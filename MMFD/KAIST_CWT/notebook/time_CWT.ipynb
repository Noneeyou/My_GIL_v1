{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca3bf53",
   "metadata": {},
   "source": [
    "# normal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f3a5e9",
   "metadata": {},
   "source": [
    "## time pyd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import subgraph, add_self_loops\n",
    "\n",
    "# === å¯¼å…¥æ¨¡å‹ç»“æ„ ===\n",
    "from models import GraphContrastiveLearner, augment_graph, summarize_graph\n",
    "\n",
    "\n",
    "# ===================== 0ï¸âƒ£ è·¯å¾„è®¾ç½® =====================\n",
    "train_graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/KAIST_CWT/normalization/train/CWT_pyg/train_graph_with_labelmask.pt\"\n",
    "val_graph_path   = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/KAIST_CWT/normalization/val/CWT_pyg/graph.pt\"\n",
    "test_graph_path  = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/KAIST_CWT/normalization/test/CWT_pyg/graph.pt\"\n",
    "\n",
    "save_dir = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT/model_save/time_CWT\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "# ===================== 1ï¸âƒ£ åŠ è½½å›¾æ•°æ® =====================\n",
    "print(\"\\n========== åŠ è½½ train å›¾ï¼ˆå¸¦ label maskï¼‰ ==========\")\n",
    "train_data = torch.load(train_graph_path)\n",
    "summarize_graph(train_data)\n",
    "\n",
    "print(\"\\n========== åŠ è½½ val å›¾ ==========\")\n",
    "val_data = torch.load(val_graph_path)\n",
    "\n",
    "print(\"\\n========== åŠ è½½ test å›¾ ==========\")\n",
    "test_data = torch.load(test_graph_path)\n",
    "\n",
    "# =====================================================\n",
    "# â­ å·¥å…·å‡½æ•°ï¼šåˆ é™¤æŒ‡å®šæ ‡ç­¾é›†åˆçš„èŠ‚ç‚¹ï¼Œå¹¶é‡å»ºå­å›¾ï¼ˆæ”¯æŒå¤šä¸ªæ ‡ç­¾ï¼‰\n",
    "# =====================================================\n",
    "def remove_labels(data, labels_to_remove):\n",
    "    \"\"\"\n",
    "    åˆ é™¤ PyG Data å¯¹è±¡ä¸­ y å±äº labels_to_remove çš„èŠ‚ç‚¹ï¼Œ\n",
    "    è‡ªåŠ¨é‡å»º xã€yã€edge_indexï¼Œå¹¶åŒæ­¥æ‰€æœ‰ maskã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        data: PyG Data\n",
    "        labels_to_remove: listï¼Œä¾‹å¦‚ [12, 13, 14]\n",
    "    \"\"\"\n",
    "    y = data.y\n",
    "    labels_to_remove = set(labels_to_remove)\n",
    "\n",
    "    # è¦ä¿ç•™çš„èŠ‚ç‚¹ = y ä¸åœ¨ remove åˆ—è¡¨ä¸­\n",
    "    keep_mask = torch.tensor([label.item() not in labels_to_remove for label in y])\n",
    "    keep_idx = keep_mask.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "    print(f\"ğŸ§¹ åˆ é™¤æ ‡ç­¾ {labels_to_remove} èŠ‚ç‚¹æ•°: {data.num_nodes - keep_idx.numel()} ä¸ª\")\n",
    "\n",
    "    # ---- å­å›¾æ–¹å¼æå–ï¼ˆè‡ªåŠ¨é‡å»º edge_indexï¼‰ ----\n",
    "    new_edge_index, _ = subgraph(\n",
    "        keep_idx,\n",
    "        data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "\n",
    "    # ---- æ„å»ºæ–°çš„ Data ----\n",
    "    new_data = Data(\n",
    "        x=data.x[keep_idx],\n",
    "        y=data.y[keep_idx],\n",
    "        edge_index=new_edge_index\n",
    "    )\n",
    "\n",
    "    # ---- è‹¥å­˜åœ¨ maskï¼Œéœ€è¦åŒæ­¥å¤„ç† ----\n",
    "    for attr in [\"train_mask\", \"val_mask\", \"test_mask\",\n",
    "                 \"train_withlabel_mask\", \"train_nolabel_mask\"]:\n",
    "        if hasattr(data, attr):\n",
    "            original_mask = getattr(data, attr)\n",
    "            filtered = original_mask[keep_idx]\n",
    "            setattr(new_data, attr, filtered)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# print(\"\\n========== æ¸…æ´— train_data ==========\")\n",
    "# train_data = remove_labels(train_data, [12,13])\n",
    "\n",
    "# print(\"\\n========== æ¸…æ´— val_data ==========\")\n",
    "# val_data = remove_labels(val_data, [12, 13])\n",
    "\n",
    "# print(\"\\n========== æ¸…æ´— test_data ==========\")\n",
    "# test_data = remove_labels(test_data, [12, 13])\n",
    "\n",
    "\n",
    "# ===================== 2ï¸âƒ£ æ·»åŠ è‡ªç¯ =====================\n",
    "\n",
    "def add_self_loop_to_graph(data):\n",
    "    # å¦‚æœå›¾æœ‰ edge_attrï¼Œä¸€å¹¶å¤„ç†ï¼›å¦åˆ™åªæ·»åŠ  edge_index\n",
    "    if hasattr(data, \"edge_attr\") and data.edge_attr is not None:\n",
    "        edge_index, edge_attr = add_self_loops(\n",
    "            data.edge_index,\n",
    "            data.edge_attr,\n",
    "            num_nodes=data.num_nodes\n",
    "        )\n",
    "        data.edge_index = edge_index\n",
    "        data.edge_attr = edge_attr\n",
    "    else:\n",
    "        edge_index, _ = add_self_loops(\n",
    "            data.edge_index,\n",
    "            num_nodes=data.num_nodes\n",
    "        )\n",
    "        data.edge_index = edge_index\n",
    "    return data\n",
    "\n",
    "# print(\"\\n========== ä¸º train å›¾æ·»åŠ è‡ªç¯ ==========\")\n",
    "# train_data = add_self_loop_to_graph(train_data)\n",
    "\n",
    "print(\"\\n========== ä¸º val å›¾æ·»åŠ è‡ªç¯ ==========\")\n",
    "val_data = add_self_loop_to_graph(val_data)\n",
    "\n",
    "print(\"\\n========== ä¸º test å›¾æ·»åŠ è‡ªç¯ ==========\")\n",
    "test_data = add_self_loop_to_graph(test_data)\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰å›¾å‡å·²æˆåŠŸåŠ å…¥è‡ªç¯ï¼\")\n",
    "\n",
    "\n",
    "\n",
    "# ===================== 2ï¸âƒ£ ç”¨ train ç»Ÿè®¡é‡åš Z-score =====================\n",
    "\n",
    "# åªåœ¨ train ä¸Šè®¡ç®—\n",
    "x_mean = train_data.x.mean(dim=0, keepdim=True)\n",
    "x_std  = train_data.x.std(dim=0, keepdim=True) + 1e-6\n",
    "\n",
    "# åº”ç”¨åˆ° train / val / test\n",
    "train_data.x = (train_data.x - x_mean) / x_std\n",
    "val_data.x   = (val_data.x   - x_mean) / x_std\n",
    "test_data.x  = (test_data.x  - x_mean) / x_std\n",
    "\n",
    "print(\"ğŸ¯ å·²å®ŒæˆåŸºäº train çš„ç»Ÿä¸€æ ‡å‡†åŒ–\")\n",
    "\n",
    "\n",
    "\n",
    "# ===================== 3ï¸âƒ£ æ„é€  batch å­å›¾ =====================\n",
    "batch_size = 1024*2\n",
    "num_nodes = train_data.num_nodes\n",
    "\n",
    "perm = torch.randperm(num_nodes)\n",
    "\n",
    "batches = []\n",
    "for i in range(0, num_nodes, batch_size):\n",
    "    node_idx = perm[i:i + batch_size]\n",
    "\n",
    "    # ---- æå–å­å›¾ï¼ˆå¿…é¡» CPUï¼‰----\n",
    "    sub_edge_index, _ = subgraph(\n",
    "        node_idx,\n",
    "        train_data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "\n",
    "    sub_x = train_data.x[node_idx]\n",
    "\n",
    "    # ---------- â­ åŠ å…¥è‡ªç¯ ----------\n",
    "    sub_edge_index, _ = add_self_loops(\n",
    "        sub_edge_index, num_nodes=sub_x.size(0)\n",
    "    )\n",
    "\n",
    "    sub_data = Data(\n",
    "        x=sub_x,\n",
    "        edge_index=sub_edge_index,\n",
    "    )\n",
    "    batches.append(sub_data)\n",
    "\n",
    "print(f\"ğŸ“Œ å…±ç”Ÿæˆ {len(batches)} ä¸ªå­å›¾ batch\")\n",
    "\n",
    "loader = DataLoader(batches, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# ===================== 4ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹ =====================\n",
    "in_dim = train_data.x.size(1)\n",
    "hidden_dim = 256\n",
    "out_dim = 256\n",
    "proj_dim = 128\n",
    "tau = 0.5\n",
    "\n",
    "model = GraphContrastiveLearner(in_dim, hidden_dim, out_dim, proj_dim, tau).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad4f8f",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 5ï¸âƒ£ Batch è®­ç»ƒ =====================\n",
    "epochs = 200\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model_path = os.path.join(save_dir, f\"KAIST_normal_pretrain_best.pt\")\n",
    "\n",
    "print(\"\\n================= ğŸš€ å¼€å§‹ Batch å¯¹æ¯”å­¦ä¹ è®­ç»ƒ =================\\n\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data[0].to(device)   # batch_size=1 â†’ å–ç¬¬ 0 ä¸ª\n",
    "\n",
    "        # ---- ç”Ÿæˆä¸¤ä»½å¢å¼ºè§†å›¾ ----\n",
    "        data1 = augment_graph(\n",
    "            batch_data,\n",
    "            feature_drop_prob=0.2,\n",
    "            edge_drop_prob=0.1,\n",
    "            noise_std=0.02\n",
    "        ).to(device)\n",
    "\n",
    "        data2 = augment_graph(\n",
    "            batch_data,\n",
    "            feature_drop_prob=0.2,\n",
    "            edge_drop_prob=0.1,\n",
    "            noise_std=0.02\n",
    "        ).to(device)\n",
    "\n",
    "        # ---- è®¡ç®— InfoNCE å¯¹æ¯”æŸå¤± ----\n",
    "        loss = model.compute_loss(\n",
    "            data1.x, data1.edge_index,\n",
    "            data2.x, data2.edge_index\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # ======== æ—¥å¿—è¾“å‡º ========\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch:03d}/{epochs}] | InfoNCE Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # ======== ä¿å­˜å½“å‰æœ€ä¼˜æ¨¡å‹ ========\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"best_loss\": best_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"config\": {\n",
    "                \"in_dim\": in_dim,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"out_dim\": out_dim,\n",
    "                \"proj_dim\": proj_dim,\n",
    "                \"tau\": tau\n",
    "            }\n",
    "        }, best_model_path)\n",
    "\n",
    "        print(f\"ğŸ’¾ [BEST MODEL UPDATED] Epoch {epoch} | Loss={total_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ===================== æœ€ç»ˆæ¨¡å‹ä¿å­˜ =====================\n",
    "final_path = os.path.join(save_dir, f\"KAIST_normal_pretrain_epoch{epochs}.pt\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"epoch\": epochs,\n",
    "    \"config\": {\n",
    "        \"in_dim\": in_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"out_dim\": out_dim,\n",
    "        \"proj_dim\": proj_dim,\n",
    "        \"tau\": tau\n",
    "    }\n",
    "}, final_path)\n",
    "\n",
    "print(f\"\\nğŸ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}\")\n",
    "print(f\"ğŸ† æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path} | best_loss={best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f936d66",
   "metadata": {},
   "source": [
    "## æ•ˆæœå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "#                â­ å• Cellï¼šä¸ä¼šæŠ¥é”™çš„æ¨¡å‹åŠ è½½ + Train/Test 3D å¯è§†åŒ– â­\n",
    "# ======================================================================\n",
    "\n",
    "# ====================== ç¯å¢ƒå¯¼å…¥ ======================\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# ====================== è·¯å¾„ ======================\n",
    "save_dir = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT/result/time_CWT\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT/model_save/time_CWT/KAIST_normal_pretrain_best.pt\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ====================== åŠ è½½è®­ç»ƒæ—¶çš„çœŸå®æ¨¡å‹ç»“æ„ ======================\n",
    "from models import GraphContrastiveLearner   \n",
    "\n",
    "# ----- åŠ è½½ checkpoint -----\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "cfg = checkpoint[\"config\"]\n",
    "\n",
    "# ----- æŒ‰è®­ç»ƒæ—¶çš„æ–¹å¼å®ä¾‹åŒ–æ¨¡å‹ï¼ˆä¸ä¼šæŠ¥é”™ ğŸ‘ï¼‰-----\n",
    "model = GraphContrastiveLearner(\n",
    "    in_dim     = cfg[\"in_dim\"],\n",
    "    hidden_dim = cfg[\"hidden_dim\"],\n",
    "    out_dim    = cfg[\"out_dim\"],\n",
    "    proj_dim   = cfg[\"proj_dim\"],\n",
    "    tau        = cfg[\"tau\"] if \"tau\" in cfg else 0.5\n",
    ").to(device)\n",
    "\n",
    "# ----- åŠ è½½æƒé‡ï¼ˆä¸ä¼šæŠ¥é”™ ğŸ‘ï¼‰-----\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… ä½¿ç”¨è®­ç»ƒæ—¶çš„åŸå§‹æ¨¡å‹è¯»å–æˆåŠŸï¼ä¸ä¼šæŠ¥é”™\")\n",
    "\n",
    "# ======================================================================\n",
    "#                â­ 1ï¸âƒ£ æå– Train å’Œ Test çš„åµŒå…¥\n",
    "# ======================================================================\n",
    "\n",
    "# ä½ å‰é¢å·²ç»ï¼štrain_data / test_data = torch.load(...)\n",
    "train_data = train_data.to(device)\n",
    "test_data  = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _, z_train = model(train_data.x, train_data.edge_index)\n",
    "    _, z_test  = model(test_data.x,  test_data.edge_index)\n",
    "\n",
    "train_emb = z_train.cpu().numpy()\n",
    "test_emb  = z_test.cpu().numpy()\n",
    "\n",
    "train_labels = train_data.y.cpu().numpy() if hasattr(train_data, \"y\") else np.zeros(train_emb.shape[0])\n",
    "test_labels  = test_data.y.cpu().numpy() if hasattr(test_data,  \"y\") else np.zeros(test_emb.shape[0])\n",
    "\n",
    "print(f\"â­ Train Embeddings Shape: {train_emb.shape}\")\n",
    "print(f\"â­ Test  Embeddings Shape: {test_emb.shape}\")\n",
    "\n",
    "# ======================================================================\n",
    "#                â­ 2ï¸âƒ£ 3D UMAP é™ç»´\n",
    "# ======================================================================\n",
    "reducer_train = umap.UMAP(\n",
    "    n_neighbors=10, min_dist=0.1, n_components=3,\n",
    "    metric=\"cosine\", random_state=42\n",
    ")\n",
    "train_umap = reducer_train.fit_transform(train_emb)\n",
    "\n",
    "reducer_test = umap.UMAP(\n",
    "    n_neighbors=10, min_dist=0.1, n_components=3,\n",
    "    metric=\"cosine\", random_state=42\n",
    ")\n",
    "test_umap = reducer_test.fit_transform(test_emb)\n",
    "\n",
    "# ======================================================================\n",
    "#                â­ 3ï¸âƒ£ ç»˜åˆ¶ Train 3D UMAP\n",
    "# ======================================================================\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    train_umap[:,0], train_umap[:,1], train_umap[:,2],\n",
    "    c=train_labels, cmap='tab20', s=8, alpha=0.85\n",
    ")\n",
    "\n",
    "ax.set_title(\"Train Graph Embeddings - 3D UMAP\")\n",
    "ax.set_xlabel(\"UMAP-1\"); ax.set_ylabel(\"UMAP-2\"); ax.set_zlabel(\"UMAP-3\")\n",
    "\n",
    "legend = plt.legend(\n",
    "    *scatter.legend_elements(num=None),\n",
    "    title=\"Labels\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), fontsize=8\n",
    ")\n",
    "ax.add_artist(legend)\n",
    "\n",
    "plt.tight_layout(rect=[0,0,0.85,1])\n",
    "plt.savefig(os.path.join(save_dir, \"train_umap_3d.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "#                â­ 4ï¸âƒ£ ç»˜åˆ¶ Test 3D UMAP\n",
    "# ======================================================================\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    test_umap[:,0], test_umap[:,1], test_umap[:,2],\n",
    "    c=test_labels, cmap='tab20', s=8, alpha=0.85\n",
    ")\n",
    "\n",
    "ax.set_title(\"Test Graph Embeddings - 3D UMAP\")\n",
    "ax.set_xlabel(\"UMAP-1\"); ax.set_ylabel(\"UMAP-2\"); ax.set_zlabel(\"UMAP-3\")\n",
    "\n",
    "legend = plt.legend(\n",
    "    *scatter.legend_elements(num=None),\n",
    "    title=\"Labels\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), fontsize=8\n",
    ")\n",
    "ax.add_artist(legend)\n",
    "\n",
    "plt.tight_layout(rect=[0,0,0.85,1])\n",
    "plt.savefig(os.path.join(save_dir, \"test_umap_3d.png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "#                â­ 5ï¸âƒ£ ä¿å­˜åµŒå…¥\n",
    "# ======================================================================\n",
    "np.save(os.path.join(save_dir, \"train_emb.npy\"), train_emb)\n",
    "np.save(os.path.join(save_dir, \"train_umap_3d.npy\"), train_umap)\n",
    "np.save(os.path.join(save_dir, \"test_emb.npy\"),  test_emb)\n",
    "np.save(os.path.join(save_dir, \"test_umap_3d.npy\"),  test_umap)\n",
    "\n",
    "print(\"\\nğŸ¯ Train / Test 3D UMAP å…¨éƒ¨å®Œæˆå¹¶ä¿å­˜ï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8162661",
   "metadata": {},
   "source": [
    "## ä¸‹æ¸¸è®­ç»ƒ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "039a42c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT/src\n",
      "ğŸš€ æå– train/val ç¼–ç ç‰¹å¾ï¼ˆEncoder forwardï¼‰...\n",
      "âœ¨ å·²å®ŒæˆåµŒå…¥å‘é‡çš„ L2 å½’ä¸€åŒ–ï¼\n",
      "ğŸ“Œ Train æœ‰æ ‡ç­¾æ ·æœ¬æ•°: 491\n",
      "ğŸ“Œ è¡¨å¾ç»´åº¦(out_dim): 256\n",
      "\n",
      "================== ğŸ”¥ å¼€å§‹ä¸‹æ¸¸åˆ†ç±»è®­ç»ƒï¼ˆå« val éªŒè¯ï¼‰ ==================\n",
      "\n",
      "Epoch 005/500 | Train Loss = 2.6204 | Val Acc = 7.35%\n",
      "Epoch 010/500 | Train Loss = 2.5517 | Val Acc = 13.24%\n",
      "Epoch 015/500 | Train Loss = 2.3985 | Val Acc = 20.59%\n",
      "Epoch 020/500 | Train Loss = 2.2065 | Val Acc = 27.94%\n",
      "Epoch 025/500 | Train Loss = 1.9692 | Val Acc = 46.69%\n",
      "Epoch 030/500 | Train Loss = 1.7543 | Val Acc = 47.43%\n",
      "Epoch 035/500 | Train Loss = 1.5553 | Val Acc = 50.74%\n",
      "Epoch 040/500 | Train Loss = 1.3805 | Val Acc = 61.40%\n",
      "Epoch 045/500 | Train Loss = 1.2258 | Val Acc = 63.24%\n",
      "Epoch 050/500 | Train Loss = 1.0937 | Val Acc = 64.34%\n",
      "Epoch 055/500 | Train Loss = 0.9843 | Val Acc = 68.01%\n",
      "Epoch 060/500 | Train Loss = 0.8937 | Val Acc = 69.85%\n",
      "Epoch 065/500 | Train Loss = 0.8185 | Val Acc = 70.22%\n",
      "Epoch 070/500 | Train Loss = 0.7555 | Val Acc = 68.75%\n",
      "Epoch 075/500 | Train Loss = 0.7041 | Val Acc = 71.32%\n",
      "Epoch 080/500 | Train Loss = 0.6596 | Val Acc = 71.32%\n",
      "Epoch 085/500 | Train Loss = 0.6229 | Val Acc = 68.75%\n",
      "Epoch 090/500 | Train Loss = 0.5895 | Val Acc = 70.96%\n",
      "Epoch 095/500 | Train Loss = 0.5633 | Val Acc = 69.49%\n",
      "Epoch 100/500 | Train Loss = 0.5399 | Val Acc = 70.22%\n",
      "Epoch 105/500 | Train Loss = 0.5200 | Val Acc = 70.96%\n",
      "Epoch 110/500 | Train Loss = 0.5067 | Val Acc = 70.22%\n",
      "Epoch 115/500 | Train Loss = 0.4888 | Val Acc = 70.22%\n",
      "Epoch 120/500 | Train Loss = 0.4783 | Val Acc = 69.49%\n",
      "Epoch 125/500 | Train Loss = 0.4681 | Val Acc = 69.49%\n",
      "Epoch 130/500 | Train Loss = 0.4574 | Val Acc = 69.49%\n",
      "Epoch 135/500 | Train Loss = 0.4512 | Val Acc = 69.85%\n",
      "Epoch 140/500 | Train Loss = 0.4427 | Val Acc = 68.75%\n",
      "Epoch 145/500 | Train Loss = 0.4365 | Val Acc = 69.12%\n",
      "Epoch 150/500 | Train Loss = 0.4373 | Val Acc = 69.85%\n",
      "Epoch 155/500 | Train Loss = 0.4275 | Val Acc = 69.49%\n",
      "Epoch 160/500 | Train Loss = 0.4211 | Val Acc = 68.75%\n",
      "Epoch 165/500 | Train Loss = 0.4163 | Val Acc = 69.12%\n",
      "Epoch 170/500 | Train Loss = 0.4126 | Val Acc = 69.49%\n",
      "Epoch 175/500 | Train Loss = 0.4097 | Val Acc = 69.85%\n",
      "Epoch 180/500 | Train Loss = 0.4058 | Val Acc = 70.22%\n",
      "Epoch 185/500 | Train Loss = 0.4041 | Val Acc = 70.22%\n",
      "Epoch 190/500 | Train Loss = 0.3991 | Val Acc = 69.85%\n",
      "Epoch 195/500 | Train Loss = 0.3970 | Val Acc = 70.22%\n",
      "Epoch 200/500 | Train Loss = 0.4035 | Val Acc = 69.85%\n",
      "Epoch 205/500 | Train Loss = 0.3951 | Val Acc = 69.49%\n",
      "Epoch 210/500 | Train Loss = 0.3910 | Val Acc = 69.85%\n",
      "Epoch 215/500 | Train Loss = 0.3885 | Val Acc = 70.59%\n",
      "Epoch 220/500 | Train Loss = 0.3857 | Val Acc = 70.22%\n",
      "Epoch 225/500 | Train Loss = 0.3834 | Val Acc = 70.22%\n",
      "Epoch 230/500 | Train Loss = 0.3814 | Val Acc = 70.96%\n",
      "Epoch 235/500 | Train Loss = 0.3849 | Val Acc = 71.32%\n",
      "Epoch 240/500 | Train Loss = 0.3787 | Val Acc = 71.32%\n",
      "Epoch 245/500 | Train Loss = 0.3782 | Val Acc = 69.49%\n",
      "Epoch 250/500 | Train Loss = 0.3750 | Val Acc = 71.32%\n",
      "Epoch 255/500 | Train Loss = 0.3739 | Val Acc = 72.43%\n",
      "Epoch 260/500 | Train Loss = 0.3766 | Val Acc = 70.22%\n",
      "Epoch 265/500 | Train Loss = 0.3803 | Val Acc = 72.43%\n",
      "Epoch 270/500 | Train Loss = 0.3723 | Val Acc = 70.22%\n",
      "Epoch 275/500 | Train Loss = 0.3714 | Val Acc = 70.22%\n",
      "Epoch 280/500 | Train Loss = 0.3682 | Val Acc = 70.96%\n",
      "Epoch 285/500 | Train Loss = 0.3651 | Val Acc = 70.59%\n",
      "Epoch 290/500 | Train Loss = 0.3631 | Val Acc = 70.59%\n",
      "Epoch 295/500 | Train Loss = 0.3820 | Val Acc = 72.43%\n",
      "Epoch 300/500 | Train Loss = 0.3745 | Val Acc = 70.96%\n",
      "Epoch 305/500 | Train Loss = 0.3631 | Val Acc = 72.06%\n",
      "Epoch 310/500 | Train Loss = 0.3673 | Val Acc = 73.16%\n",
      "Epoch 315/500 | Train Loss = 0.3611 | Val Acc = 73.16%\n",
      "Epoch 320/500 | Train Loss = 0.3557 | Val Acc = 72.06%\n",
      "Epoch 325/500 | Train Loss = 0.3548 | Val Acc = 69.85%\n",
      "Epoch 330/500 | Train Loss = 0.3539 | Val Acc = 69.85%\n",
      "Epoch 335/500 | Train Loss = 0.3528 | Val Acc = 71.32%\n",
      "Epoch 340/500 | Train Loss = 0.3544 | Val Acc = 69.85%\n",
      "Epoch 345/500 | Train Loss = 0.3543 | Val Acc = 70.22%\n",
      "Epoch 350/500 | Train Loss = 0.3643 | Val Acc = 72.06%\n",
      "Epoch 355/500 | Train Loss = 0.3582 | Val Acc = 71.69%\n",
      "Epoch 360/500 | Train Loss = 0.3556 | Val Acc = 71.69%\n",
      "Epoch 365/500 | Train Loss = 0.3549 | Val Acc = 70.59%\n",
      "Epoch 370/500 | Train Loss = 0.3501 | Val Acc = 70.96%\n",
      "Epoch 375/500 | Train Loss = 0.3460 | Val Acc = 72.06%\n",
      "Epoch 380/500 | Train Loss = 0.3455 | Val Acc = 70.22%\n",
      "Epoch 385/500 | Train Loss = 0.3449 | Val Acc = 70.59%\n",
      "Epoch 390/500 | Train Loss = 0.3441 | Val Acc = 70.22%\n",
      "Epoch 395/500 | Train Loss = 0.3510 | Val Acc = 72.06%\n",
      "Epoch 400/500 | Train Loss = 0.3445 | Val Acc = 72.43%\n",
      "Epoch 405/500 | Train Loss = 0.3466 | Val Acc = 70.96%\n",
      "Epoch 410/500 | Train Loss = 0.3470 | Val Acc = 70.22%\n",
      "Epoch 415/500 | Train Loss = 0.3421 | Val Acc = 69.85%\n",
      "Epoch 420/500 | Train Loss = 0.3446 | Val Acc = 72.79%\n",
      "Epoch 425/500 | Train Loss = 0.3387 | Val Acc = 71.32%\n",
      "Epoch 430/500 | Train Loss = 0.3410 | Val Acc = 70.22%\n",
      "Epoch 435/500 | Train Loss = 0.3366 | Val Acc = 70.22%\n",
      "Epoch 440/500 | Train Loss = 0.3393 | Val Acc = 72.43%\n",
      "Epoch 445/500 | Train Loss = 0.3375 | Val Acc = 70.59%\n",
      "Epoch 450/500 | Train Loss = 0.3346 | Val Acc = 72.43%\n",
      "Epoch 455/500 | Train Loss = 0.3482 | Val Acc = 71.69%\n",
      "Epoch 460/500 | Train Loss = 0.3408 | Val Acc = 71.32%\n",
      "Epoch 465/500 | Train Loss = 0.3366 | Val Acc = 70.59%\n",
      "Epoch 470/500 | Train Loss = 0.3326 | Val Acc = 71.32%\n",
      "Epoch 475/500 | Train Loss = 0.3330 | Val Acc = 70.22%\n",
      "Epoch 480/500 | Train Loss = 0.3328 | Val Acc = 70.22%\n",
      "Epoch 485/500 | Train Loss = 0.3299 | Val Acc = 72.79%\n",
      "Epoch 490/500 | Train Loss = 0.3315 | Val Acc = 72.43%\n",
      "Epoch 495/500 | Train Loss = 0.3369 | Val Acc = 70.22%\n",
      "Epoch 500/500 | Train Loss = 0.3345 | Val Acc = 70.59%\n",
      "\n",
      "ğŸ ä¸‹æ¸¸åˆ†ç±»å™¨è®­ç»ƒå®Œæˆï¼\n",
      "ğŸŒŸ æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡ï¼š73.90%\n",
      "ğŸ’¾ æœ€ä½³ï¼ˆVal Accæœ€é«˜ï¼‰åˆ†ç±»å™¨å·²ä¿å­˜ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT/model_save/time_CWT/downstream/downstream_classifier_val_best.pt\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "#        â­ å• Cellï¼šä½¿ç”¨ train_withlabel_mask æ•´å›¾è®­ç»ƒä¸‹æ¸¸åˆ†ç±»å™¨ï¼ˆå« val éªŒè¯é›†ï¼‰ â­\n",
    "# ======================================================================\n",
    "%run ../_init_path.py\n",
    "from models import GraphContrastiveLearner, DownstreamKANClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ======================================================================\n",
    "# 1. æå– trainã€val çš„ encoder è¡¨å¾\n",
    "# ======================================================================\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "val_data   = val_data.to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"ğŸš€ æå– train/val ç¼–ç ç‰¹å¾ï¼ˆEncoder forwardï¼‰...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    h_train, _ = model(train_data.x, train_data.edge_index)\n",
    "    h_val,   _ = model(val_data.x,   val_data.edge_index)\n",
    "\n",
    "h_train = h_train.detach()\n",
    "h_val   = h_val.detach()\n",
    "\n",
    "# ======================================================================\n",
    "#          â­ å¯¹ä¸Šæ¸¸ç”Ÿæˆçš„åµŒå…¥è¡¨è¾¾ h_train / h_test åš L2 å½’ä¸€åŒ–\n",
    "# ======================================================================\n",
    "\n",
    "def l2_normalize(h, eps=1e-8):\n",
    "    return h / (h.norm(dim=1, keepdim=True) + eps)\n",
    "\n",
    "h_train = l2_normalize(h_train)\n",
    "h_val  = l2_normalize(h_val)\n",
    "\n",
    "print(\"âœ¨ å·²å®ŒæˆåµŒå…¥å‘é‡çš„ L2 å½’ä¸€åŒ–ï¼\")\n",
    "\n",
    "# ---- train ----\n",
    "y_train = train_data.y\n",
    "mask_train = train_data.train_withlabel_mask\n",
    "idx_train = mask_train.nonzero(as_tuple=False).view(-1)\n",
    "h_train_labeled = h_train[idx_train]\n",
    "y_train_labeled = y_train[idx_train]\n",
    "\n",
    "# ---- valï¼ˆå…¨éƒ¨ val èŠ‚ç‚¹ç”¨äºéªŒè¯ï¼‰ ----\n",
    "y_val = val_data.y\n",
    "\n",
    "print(f\"ğŸ“Œ Train æœ‰æ ‡ç­¾æ ·æœ¬æ•°: {h_train_labeled.shape[0]}\")\n",
    "print(f\"ğŸ“Œ è¡¨å¾ç»´åº¦(out_dim): {h_train_labeled.shape[1]}\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 2. å®šä¹‰åˆ†ç±»å™¨\n",
    "# ======================================================================\n",
    "\n",
    "num_features = h_train_labeled.size(1)\n",
    "num_classes = int(max(y_train_labeled.max(), y_val.max()).item() + 1)\n",
    "\n",
    "classifier = DownstreamKANClassifier(\n",
    "    in_dim=num_features,\n",
    "    num_classes=num_classes,\n",
    "    hidden_dim=128,\n",
    "    num_knots=8\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 3. æ•´å›¾è®­ç»ƒ + éªŒè¯é›†è¯„ä¼°ï¼ˆä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰\n",
    "# ======================================================================\n",
    "\n",
    "epochs = 500\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "print(\"\\n================== ğŸ”¥ å¼€å§‹ä¸‹æ¸¸åˆ†ç±»è®­ç»ƒï¼ˆå« val éªŒè¯ï¼‰ ==================\\n\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # ---- Train step ----\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits_train = classifier(h_train_labeled)\n",
    "    loss = F.cross_entropy(logits_train, y_train_labeled)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ---- Val step ----\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_val = classifier(h_val)\n",
    "        pred_val = logits_val.argmax(dim=1)\n",
    "        acc_val = (pred_val == y_val).float().mean().item()\n",
    "\n",
    "    # ---- æ‰“å° ----\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:03d}/{epochs} | Train Loss = {loss.item():.4f} | Val Acc = {acc_val*100:.2f}%\")\n",
    "\n",
    "    # ---- ä¿å­˜æœ€ä½³æ¨¡å‹ ----\n",
    "    if acc_val > best_val_acc:\n",
    "        best_val_acc = acc_val\n",
    "        best_state = {\n",
    "            \"state_dict\": classifier.state_dict(),\n",
    "            \"in_dim\": num_features,\n",
    "            \"num_classes\": num_classes\n",
    "        }\n",
    "\n",
    "print(\"\\nğŸ ä¸‹æ¸¸åˆ†ç±»å™¨è®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"ğŸŒŸ æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡ï¼š{best_val_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "# 4. ä¿å­˜éªŒè¯é›†æœ€ä¼˜æ¨¡å‹\n",
    "# ======================================================================\n",
    "\n",
    "save_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT/model_save/time_CWT/downstream/downstream_classifier_val_best.pt\"\n",
    "\n",
    "torch.save(best_state, save_path)\n",
    "\n",
    "print(f\"ğŸ’¾ æœ€ä½³ï¼ˆVal Accæœ€é«˜ï¼‰åˆ†ç±»å™¨å·²ä¿å­˜ï¼š{save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663731ab",
   "metadata": {},
   "source": [
    "## æœ€ç»ˆaccæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dff3945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ æ¨¡å‹è·¯å¾„å·²è®¾ç½®\n",
      "âœ… ä¸Šæ¸¸æ¨¡å‹åŠ è½½æˆåŠŸï¼\n",
      "âœ… ä¸‹æ¸¸åˆ†ç±»å™¨åŠ è½½æˆåŠŸï¼\n",
      "ğŸ“Œ Train/Test å›¾æ•°æ®å·²åŠ è½½è‡³ GPU/CPU\n",
      "ğŸ”§ å·²æå–ä¸Šæ¸¸ç¼–ç ç‰¹å¾ h_train / h_test\n",
      "\n",
      "ğŸ¯ Train Accuracy (train_withlabel_mask) = 86.15%\n",
      "ğŸ¯ Test Accuracy  (all test nodes) = 82.30%\n",
      "\n",
      "ğŸ å…¨éƒ¨ accuracy è¯„ä¼°å®Œæˆï¼\n",
      "\n",
      "====================== TRAIN METRICS ======================\n",
      "Accuracy : 86.15%\n",
      "Precision: 87.60%\n",
      "Recall   : 87.06%\n",
      "F1-score : 86.82%\n",
      "===========================================================\n",
      "\n",
      "======================= TEST METRICS =======================\n",
      "Accuracy : 82.30%\n",
      "Precision: 81.63%\n",
      "Recall   : 82.36%\n",
      "F1-score : 81.82%\n",
      "===========================================================\n",
      "\n",
      "============== TRAIN WRONG PREDICTIONS ==============\n",
      "âŒ é”™è¯¯æ ·æœ¬æ•°é‡ï¼š68 / 491\n",
      "Index  348 | True = 11 | Pred = 14\n",
      "Index  349 | True = 11 | Pred = 14\n",
      "Index  350 | True = 11 | Pred = 14\n",
      "Index  351 | True = 11 | Pred = 14\n",
      "Index  352 | True = 11 | Pred = 14\n",
      "Index  353 | True = 11 | Pred = 14\n",
      "Index  354 | True = 11 | Pred = 14\n",
      "Index  355 | True = 11 | Pred = 14\n",
      "Index  363 | True = 11 | Pred = 13\n",
      "Index  364 | True = 11 | Pred = 13\n",
      "Index  371 | True = 11 | Pred = 13\n",
      "Index  372 | True = 11 | Pred = 12\n",
      "Index  373 | True = 11 | Pred = 13\n",
      "Index  378 | True = 12 | Pred = 11\n",
      "Index  381 | True = 12 | Pred = 14\n",
      "Index  382 | True = 12 | Pred = 14\n",
      "Index  383 | True = 12 | Pred = 14\n",
      "Index  386 | True = 12 | Pred = 11\n",
      "Index  387 | True = 12 | Pred = 11\n",
      "Index  388 | True = 12 | Pred = 11\n",
      "... (å…± 68 ä¸ªé”™æ ·æœ¬ï¼Œåªæ˜¾ç¤ºå‰ 20 ä¸ª)\n",
      "\n",
      "ğŸ” Train æ¯ç±»é”™è¯¯ç»Ÿè®¡ï¼š(çœŸæ ‡ç­¾ -> é¢„æµ‹æ ‡ç­¾)\n",
      "  11 â†’ 14 : 8 æ¬¡\n",
      "  11 â†’ 13 : 4 æ¬¡\n",
      "  11 â†’ 12 : 1 æ¬¡\n",
      "  12 â†’ 11 : 6 æ¬¡\n",
      "  12 â†’ 14 : 9 æ¬¡\n",
      "  12 â†’ 13 : 8 æ¬¡\n",
      "  13 â†’ 12 : 8 æ¬¡\n",
      "  13 â†’ 14 : 17 æ¬¡\n",
      "  14 â†’ 13 : 5 æ¬¡\n",
      "  14 â†’ 12 : 2 æ¬¡\n",
      "\n",
      "============== TEST WRONG PREDICTIONS ===============\n",
      "âŒ é”™è¯¯æ ·æœ¬æ•°é‡ï¼š145 / 819\n",
      "Index   34 | True = 1 | Pred = 11\n",
      "Index   35 | True = 1 | Pred = 11\n",
      "Index   36 | True = 1 | Pred = 11\n",
      "Index   37 | True = 1 | Pred = 11\n",
      "Index   54 | True = 1 | Pred = 5\n",
      "Index   55 | True = 1 | Pred = 5\n",
      "Index   56 | True = 1 | Pred = 5\n",
      "Index   57 | True = 1 | Pred = 5\n",
      "Index  116 | True = 3 | Pred = 2\n",
      "Index  175 | True = 4 | Pred = 3\n",
      "Index  176 | True = 4 | Pred = 3\n",
      "Index  233 | True = 5 | Pred = 4\n",
      "Index  234 | True = 5 | Pred = 4\n",
      "Index  291 | True = 5 | Pred = 6\n",
      "Index  292 | True = 5 | Pred = 6\n",
      "Index  409 | True = 8 | Pred = 7\n",
      "Index  410 | True = 8 | Pred = 7\n",
      "Index  411 | True = 8 | Pred = 7\n",
      "Index  580 | True = 10 | Pred = 9\n",
      "Index  581 | True = 10 | Pred = 9\n",
      "... (å…± 145 ä¸ªé”™æ ·æœ¬ï¼Œåªæ˜¾ç¤ºå‰ 20 ä¸ª)\n",
      "\n",
      "ğŸ” Test æ¯ç±»é”™è¯¯ç»Ÿè®¡ï¼š(çœŸæ ‡ç­¾ -> é¢„æµ‹æ ‡ç­¾)\n",
      "  1 â†’ 11 : 4 æ¬¡\n",
      "  1 â†’ 5 : 4 æ¬¡\n",
      "  3 â†’ 2 : 1 æ¬¡\n",
      "  4 â†’ 3 : 2 æ¬¡\n",
      "  5 â†’ 4 : 2 æ¬¡\n",
      "  5 â†’ 6 : 2 æ¬¡\n",
      "  8 â†’ 7 : 3 æ¬¡\n",
      "  10 â†’ 9 : 4 æ¬¡\n",
      "  11 â†’ 9 : 3 æ¬¡\n",
      "  11 â†’ 12 : 4 æ¬¡\n",
      "  12 â†’ 11 : 17 æ¬¡\n",
      "  12 â†’ 1 : 5 æ¬¡\n",
      "  12 â†’ 13 : 21 æ¬¡\n",
      "  13 â†’ 12 : 18 æ¬¡\n",
      "  13 â†’ 14 : 26 æ¬¡\n",
      "  14 â†’ 13 : 6 æ¬¡\n",
      "  14 â†’ 12 : 23 æ¬¡\n",
      "\n",
      "ğŸ é”™è¯¯æ ·æœ¬ç»Ÿè®¡å®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16826/3897915642.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(pretrain_path, map_location=device)\n",
      "/tmp/ipykernel_16826/3897915642.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  clf_ckpt = torch.load(classifier_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "#    â­ å• Cellï¼šåŠ è½½ä¸Šæ¸¸æ¨¡å‹ + ä¸‹æ¸¸åˆ†ç±»å™¨ï¼Œå¹¶è®¡ç®— Train/Test Accuracy â­\n",
    "# ======================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ====================== æ¨¡å‹è·¯å¾„ ======================\n",
    "pretrain_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT/model_save/time_CWT/KAIST_normal_pretrain_best.pt\"\n",
    "\n",
    "classifier_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_CWT/model_save/time_CWT/downstream/downstream_classifier_val_best.pt\"\n",
    "\n",
    "print(\"ğŸ“Œ æ¨¡å‹è·¯å¾„å·²è®¾ç½®\")\n",
    "\n",
    "\n",
    "# ====================== åŠ è½½æ¨¡å‹ç»“æ„ ======================\n",
    "from models import GraphContrastiveLearner, DownstreamKANClassifier\n",
    "\n",
    "# ------- åŠ è½½ä¸Šæ¸¸ GCL æ¨¡å‹ -------\n",
    "ckpt = torch.load(pretrain_path, map_location=device)\n",
    "cfg = ckpt[\"config\"]\n",
    "\n",
    "model = GraphContrastiveLearner(\n",
    "    in_dim     = cfg[\"in_dim\"],\n",
    "    hidden_dim = cfg[\"hidden_dim\"],\n",
    "    out_dim    = cfg[\"out_dim\"],\n",
    "    proj_dim   = cfg[\"proj_dim\"],\n",
    "    tau        = cfg[\"tau\"] if \"tau\" in cfg else 0.5\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])   # â­ ä¸ä¼šæŠ¥é”™çš„åŠ è½½æ–¹å¼\n",
    "model.eval()\n",
    "print(\"âœ… ä¸Šæ¸¸æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "\n",
    "\n",
    "# ------- åŠ è½½ä¸‹æ¸¸åˆ†ç±»å™¨ -------\n",
    "clf_ckpt = torch.load(classifier_path, map_location=device)\n",
    "in_dim = clf_ckpt[\"in_dim\"]\n",
    "num_classes = clf_ckpt[\"num_classes\"]\n",
    "\n",
    "classifier = DownstreamKANClassifier(\n",
    "    in_dim=in_dim,\n",
    "    num_classes=num_classes,\n",
    "    hidden_dim=128,\n",
    "    num_knots=8\n",
    ").to(device)\n",
    "\n",
    "classifier.load_state_dict(clf_ckpt[\"state_dict\"])\n",
    "classifier.eval()\n",
    "print(\"âœ… ä¸‹æ¸¸åˆ†ç±»å™¨åŠ è½½æˆåŠŸï¼\")\n",
    "\n",
    "\n",
    "# ====================== åŠ è½½ Train / Test å›¾ ======================\n",
    "# ä½ åœ¨ notebook ä¸­åº”è¯¥å·²ç»æ‰‹åŠ¨åŠ è½½å¥½äº†ï¼š\n",
    "# train_data = torch.load(...)\n",
    "# test_data = torch.load(...)\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "print(\"ğŸ“Œ Train/Test å›¾æ•°æ®å·²åŠ è½½è‡³ GPU/CPU\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#          â­ 1ï¸âƒ£ æå– Train/Test çš„ GCL encoder è¡¨å¾ h\n",
    "# ======================================================================\n",
    "with torch.no_grad():\n",
    "    h_train, _ = model(train_data.x, train_data.edge_index)\n",
    "    h_test,  _ = model(test_data.x,  test_data.edge_index)\n",
    "\n",
    "print(\"ğŸ”§ å·²æå–ä¸Šæ¸¸ç¼–ç ç‰¹å¾ h_train / h_test\")\n",
    "h_train  = l2_normalize(h_train)\n",
    "h_test  = l2_normalize(h_test)\n",
    "\n",
    "# ======================================================================\n",
    "#          â­ 2ï¸âƒ£ Train Accuracyï¼ˆä»… train_withlabel_maskï¼‰\n",
    "# ======================================================================\n",
    "train_mask = train_data.train_withlabel_mask\n",
    "train_idx = train_mask.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "h_labeled = h_train[train_idx]\n",
    "y_labeled = train_data.y[train_idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_train = classifier(h_labeled)\n",
    "    pred_train = logits_train.argmax(dim=1)\n",
    "\n",
    "acc_train = (pred_train == y_labeled).float().mean().item()\n",
    "\n",
    "print(f\"\\nğŸ¯ Train Accuracy (train_withlabel_mask) = {acc_train*100:.2f}%\")\n",
    "\n",
    "# ======================================================================\n",
    "#          â­ 3ï¸âƒ£ Test Accuracyï¼ˆå…¨éƒ¨ test èŠ‚ç‚¹ï¼‰\n",
    "# ======================================================================\n",
    "with torch.no_grad():\n",
    "    logits_test = classifier(h_test)\n",
    "    pred_test = logits_test.argmax(dim=1)\n",
    "\n",
    "acc_test = (pred_test == test_data.y).float().mean().item()\n",
    "\n",
    "print(f\"ğŸ¯ Test Accuracy  (all test nodes) = {acc_test*100:.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ å…¨éƒ¨ accuracy è¯„ä¼°å®Œæˆï¼\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# ====================== â­ Train metrics ======================\n",
    "y_true_train = y_labeled.cpu().numpy()\n",
    "y_pred_train = pred_train.cpu().numpy()\n",
    "\n",
    "train_acc  = accuracy_score(y_true_train, y_pred_train)\n",
    "train_prec = precision_score(y_true_train, y_pred_train, average=\"macro\", zero_division=0)\n",
    "train_reca = recall_score(y_true_train, y_pred_train, average=\"macro\", zero_division=0)\n",
    "train_f1   = f1_score(y_true_train, y_pred_train, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n====================== TRAIN METRICS ======================\")\n",
    "print(f\"Accuracy : {train_acc*100:.2f}%\")\n",
    "print(f\"Precision: {train_prec*100:.2f}%\")\n",
    "print(f\"Recall   : {train_reca*100:.2f}%\")\n",
    "print(f\"F1-score : {train_f1*100:.2f}%\")\n",
    "print(\"===========================================================\\n\")\n",
    "\n",
    "\n",
    "# ====================== â­ Test metrics ======================\n",
    "y_true_test = test_data.y.cpu().numpy()\n",
    "y_pred_test = pred_test.cpu().numpy()\n",
    "\n",
    "test_acc  = accuracy_score(y_true_test, y_pred_test)\n",
    "test_prec = precision_score(y_true_test, y_pred_test, average=\"macro\", zero_division=0)\n",
    "test_reca = recall_score(y_true_test, y_pred_test, average=\"macro\", zero_division=0)\n",
    "test_f1   = f1_score(y_true_test, y_pred_test, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"======================= TEST METRICS =======================\")\n",
    "print(f\"Accuracy : {test_acc*100:.2f}%\")\n",
    "print(f\"Precision: {test_prec*100:.2f}%\")\n",
    "print(f\"Recall   : {test_reca*100:.2f}%\")\n",
    "print(f\"F1-score : {test_f1*100:.2f}%\")\n",
    "print(\"===========================================================\\n\")\n",
    "# ======================================================================\n",
    "#          â­ 4ï¸âƒ£ ç»Ÿè®¡é¢„æµ‹é”™è¯¯çš„æ ·æœ¬ï¼ˆçœŸå®æ ‡ç­¾ vs é¢„æµ‹æ ‡ç­¾ï¼‰\n",
    "# ======================================================================\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ========== Train é”™è¯¯æ ·æœ¬ ==========\n",
    "wrong_train_idx = np.where(y_pred_train != y_true_train)[0]\n",
    "\n",
    "print(\"============== TRAIN WRONG PREDICTIONS ==============\")\n",
    "print(f\"âŒ é”™è¯¯æ ·æœ¬æ•°é‡ï¼š{len(wrong_train_idx)} / {len(y_true_train)}\")\n",
    "\n",
    "# æ‰“å°å‰ 20 ä¸ªé”™è¯¯æ ·æœ¬ï¼ˆé¿å…å¤ªé•¿ï¼‰\n",
    "max_show = 20\n",
    "for i in wrong_train_idx[:max_show]:\n",
    "    print(f\"Index {i:4d} | True = {y_true_train[i]} | Pred = {y_pred_train[i]}\")\n",
    "if len(wrong_train_idx) > max_show:\n",
    "    print(f\"... (å…± {len(wrong_train_idx)} ä¸ªé”™æ ·æœ¬ï¼Œåªæ˜¾ç¤ºå‰ {max_show} ä¸ª)\")\n",
    "\n",
    "# æ¯ç±»é”™è¯¯ç»Ÿè®¡\n",
    "train_wrong_pairs = [(int(y_true_train[i]), int(y_pred_train[i])) for i in wrong_train_idx]\n",
    "train_wrong_count = Counter(train_wrong_pairs)\n",
    "\n",
    "print(\"\\nğŸ” Train æ¯ç±»é”™è¯¯ç»Ÿè®¡ï¼š(çœŸæ ‡ç­¾ -> é¢„æµ‹æ ‡ç­¾)\")\n",
    "for (t, p), cnt in train_wrong_count.items():\n",
    "    print(f\"  {t} â†’ {p} : {cnt} æ¬¡\")\n",
    "\n",
    "\n",
    "# ========== Test é”™è¯¯æ ·æœ¬ ==========\n",
    "wrong_test_idx = np.where(y_pred_test != y_true_test)[0]\n",
    "\n",
    "print(\"\\n============== TEST WRONG PREDICTIONS ===============\")\n",
    "print(f\"âŒ é”™è¯¯æ ·æœ¬æ•°é‡ï¼š{len(wrong_test_idx)} / {len(y_true_test)}\")\n",
    "\n",
    "for i in wrong_test_idx[:max_show]:\n",
    "    print(f\"Index {i:4d} | True = {y_true_test[i]} | Pred = {y_pred_test[i]}\")\n",
    "if len(wrong_test_idx) > max_show:\n",
    "    print(f\"... (å…± {len(wrong_test_idx)} ä¸ªé”™æ ·æœ¬ï¼Œåªæ˜¾ç¤ºå‰ {max_show} ä¸ª)\")\n",
    "\n",
    "# æ¯ç±»é”™è¯¯ç»Ÿè®¡\n",
    "test_wrong_pairs = [(int(y_true_test[i]), int(y_pred_test[i])) for i in wrong_test_idx]\n",
    "test_wrong_count = Counter(test_wrong_pairs)\n",
    "\n",
    "print(\"\\nğŸ” Test æ¯ç±»é”™è¯¯ç»Ÿè®¡ï¼š(çœŸæ ‡ç­¾ -> é¢„æµ‹æ ‡ç­¾)\")\n",
    "for (t, p), cnt in test_wrong_count.items():\n",
    "    print(f\"  {t} â†’ {p} : {cnt} æ¬¡\")\n",
    "\n",
    "print(\"\\nğŸ é”™è¯¯æ ·æœ¬ç»Ÿè®¡å®Œæˆï¼\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
