{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fb9667",
   "metadata": {},
   "source": [
    "# è®¾ç½®æ•°æ®æ–‡ä»¶å­˜æ”¾æ ¹ç›®å½•ï¼ˆä¸ç›´æ¥æ”¾åœ¨è¯¥é¡¹ç›®ä¸­ï¼Œè¯¥é¡¹ç›®æœ‰githubä»“åº“ï¼Œé¿å…å¤§å†…å­˜æ•°æ®å½±å“ä»£ç æ¨é€ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc098e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_file_adr = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b3c79",
   "metadata": {},
   "source": [
    "# å»ºç«‹æ˜ å°„è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ========================\n",
    "# è·¯å¾„é…ç½®\n",
    "# ========================\n",
    "# boot_file_adr = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST\"\n",
    "raw_csv_dir = os.path.join(boot_file_adr, \"raw_csv\")\n",
    "save_dir = os.path.join(boot_file_adr, \"mapping_table\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"KAIST_vibration_mapping.csv\")\n",
    "\n",
    "# ========================\n",
    "# è·å–æ–‡ä»¶åˆ—è¡¨\n",
    "# ========================\n",
    "csv_files = sorted([f for f in os.listdir(raw_csv_dir) if f.endswith(\".csv\")])\n",
    "\n",
    "# ========================\n",
    "# æ ‡ç­¾æ˜ å°„ï¼ˆNormal=1ï¼Œä¸åŒè½¬é€Ÿ/æ•…éšœç‹¬ç«‹ç¼–å·ï¼‰\n",
    "# ========================\n",
    "mapping_rules = {\n",
    "    \"Normal\": 1,\n",
    "    \"BPFI_03\": 2,\n",
    "    \"BPFI_10\": 3,\n",
    "    \"BPFI_30\": 4,\n",
    "    \"BPFO_03\": 5,\n",
    "    \"BPFO_10\": 6,\n",
    "    \"BPFO_30\": 7,\n",
    "    \"Misalign_01\": 8,\n",
    "    \"Misalign_03\": 9,\n",
    "    \"Misalign_05\": 10,\n",
    "    \"Unbalance_0583mg\": 11,\n",
    "    \"Unbalance_1169mg\": 12,\n",
    "    \"Unbalance_1751mg\": 13,\n",
    "    \"Unbalance_2239mg\": 14,\n",
    "    \"Unbalance_3318mg\": 15,\n",
    "}\n",
    "\n",
    "# ========================\n",
    "# ç”Ÿæˆæ˜ å°„è¡¨\n",
    "# ========================\n",
    "records = []\n",
    "for f in csv_files:\n",
    "    matched_label = None\n",
    "    for key, label in mapping_rules.items():\n",
    "        if key in f:\n",
    "            matched_label = label\n",
    "            break\n",
    "    if matched_label is None:\n",
    "        matched_label = -1\n",
    "    records.append({\n",
    "        \"filename\": f,\n",
    "        \"label\": matched_label\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"âœ… å·²ç”Ÿæˆæ˜ å°„è¡¨ï¼š{save_path}\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cade51",
   "metadata": {},
   "source": [
    "# æ•°æ®æ··åˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befbc6c3",
   "metadata": {},
   "source": [
    "## å…¨ç±»åˆ«æ··åˆï¼Œç”¨äºåŸºæœ¬æ¨¡å‹è®­ç»ƒè¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db9b2c",
   "metadata": {},
   "source": [
    "### mixed data->csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a81ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“˜ æ˜ å°„è¡¨å·²è¯»å–ï¼Œå…± 15 æ¡è®°å½•\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 1ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 1 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_Normal.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 1 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 2ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 2 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_BPFI_03.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 2 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 3ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 3 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_BPFI_10.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 3 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 4ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 4 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_BPFI_30.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 4 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 5ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 5 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_BPFO_03.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 5 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 6ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 6 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_BPFO_10.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 6 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 7ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 7 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_BPFO_30.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 7 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 8ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 8 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_Misalign_01.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 8 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 9ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 9 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_Misalign_03.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 9 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 10ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 10 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_Misalign_05.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 10 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 11ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 11 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_Unbalance_0583mg.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 11 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 12ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 12 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_Unbalance_1169mg.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 12 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 13ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 13 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_Unbalance_1751mg.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 13 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« 14ï¼Œç›®æ ‡æŠ½å– 5000 æ¡\n",
      "==============================\n",
      "ğŸ“Š å½“å‰ç±»åˆ« 14 å·²æœ‰ 0 æ¡\n",
      "ğŸ” è¿˜éœ€ 5000 æ¡æ ·æœ¬\n",
      "ğŸ“¦ ä» 0Nm_Unbalance_2239mg.csv æŠ½å– 5000 æ¡ã€‚\n",
      "ğŸ‰ ç±»åˆ« 14 å·²æŠ½æ»¡ 5000 æ¡ã€‚\n",
      "\n",
      "==============================\n",
      "ğŸ æ‰€æœ‰ç±»åˆ«æŠ½æ ·å®Œæˆï¼\n",
      "ğŸ“ å·²ä¿å­˜ï¼š/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/mixed_allclass.csv\n",
      "ğŸ“ ä½¿ç”¨è®°å½•ï¼š/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode//used_indices.csv\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== 1ï¸âƒ£ å‚æ•°è®¾ç½® =====================\n",
    "raw_dir = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/raw_csv\"\n",
    "mapping_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/mapping_table/KAIST_vibration_mapping.csv\"\n",
    "save_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/mixed_allclass.csv\"\n",
    "used_index_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode//used_indices.csv\"\n",
    "\n",
    "# ========== ä½ éœ€è¦çš„æ¯ç±»æŠ½å–æ•°é‡ ==========\n",
    "sample_dict = {\n",
    "    1: 5000,\n",
    "    2: 5000,\n",
    "    3: 5000,\n",
    "    4: 5000,\n",
    "    5: 5000,\n",
    "    6: 5000,\n",
    "    7: 5000,\n",
    "    8: 5000,\n",
    "    9: 5000,\n",
    "    10: 5000,\n",
    "    11: 5000,\n",
    "    12: 5000,\n",
    "    13: 5000,\n",
    "    14: 5000,\n",
    "}\n",
    "\n",
    "# ===================== 2ï¸âƒ£ è¯»å–æ˜ å°„è¡¨ =====================\n",
    "mapping = pd.read_csv(mapping_path)\n",
    "print(f\"ğŸ“˜ æ˜ å°„è¡¨å·²è¯»å–ï¼Œå…± {len(mapping)} æ¡è®°å½•\")\n",
    "\n",
    "# ===================== 3ï¸âƒ£ åŠ è½½å·²ä½¿ç”¨ç´¢å¼• =====================\n",
    "if os.path.exists(used_index_path):\n",
    "    used_indices = pd.read_csv(used_index_path)\n",
    "else:\n",
    "    used_indices = pd.DataFrame(columns=[\"source_file\", \"index\", \"label\"])\n",
    "\n",
    "# ===================== 4ï¸âƒ£ åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶ =====================\n",
    "if os.path.exists(save_path):\n",
    "    os.remove(save_path)\n",
    "    print(\"âš™ï¸ å·²æ¸…ç©ºæ—§çš„ mixed_allclass.csv\")\n",
    "first_write = True\n",
    "\n",
    "# ===================== 5ï¸âƒ£ å®é™…æŠ½æ ·å¼€å§‹ =====================\n",
    "# éå†æ¯ä¸€ç±»\n",
    "for label, target_n in sample_dict.items():\n",
    "\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"ğŸ¯ å¼€å§‹å¤„ç†ç±»åˆ« {label}ï¼Œç›®æ ‡æŠ½å– {target_n} æ¡\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    # ç»Ÿè®¡å½“å‰ç±»åˆ«å·²æŠ½å–å¤šå°‘\n",
    "    already_used = used_indices.loc[used_indices[\"label\"] == label].shape[0]\n",
    "    print(f\"ğŸ“Š å½“å‰ç±»åˆ« {label} å·²æœ‰ {already_used} æ¡\")\n",
    "\n",
    "    if already_used >= target_n:\n",
    "        print(f\"âœ… ç±»åˆ« {label} æ ·æœ¬å·²è¶³å¤Ÿï¼ˆ{already_used} >= {target_n}ï¼‰ï¼Œè·³è¿‡æœ¬ç±»ã€‚\")\n",
    "        continue\n",
    "\n",
    "    # è¿˜éœ€è¦æŠ½çš„æ•°\n",
    "    remain = target_n - already_used\n",
    "    print(f\"ğŸ” è¿˜éœ€ {remain} æ¡æ ·æœ¬\")\n",
    "\n",
    "    # éå†æ‰€æœ‰æ–‡ä»¶ï¼Œå±äºè¯¥ç±»çš„éƒ½å°è¯•æŠ½å–\n",
    "    for fname in os.listdir(raw_dir):\n",
    "\n",
    "        if not fname.endswith(\".csv\"):\n",
    "            continue\n",
    "        if \"2Nm\" in fname:   # æ’é™¤ 2Nm æ–‡ä»¶\n",
    "            continue\n",
    "\n",
    "        # æ‰¾å½“å‰æ–‡ä»¶çš„æ ‡ç­¾\n",
    "        entry = mapping.loc[mapping[\"filename\"] == fname]\n",
    "        if entry.empty:\n",
    "            continue\n",
    "\n",
    "        file_label = int(entry[\"label\"].values[0])\n",
    "        if file_label != label:\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(raw_dir, fname)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df[\"label\"] = label\n",
    "        df[\"source_file\"] = fname\n",
    "\n",
    "        # å½“å‰æ–‡ä»¶æœªä½¿ç”¨çš„æ ·æœ¬\n",
    "        used_idx_this_file = used_indices.loc[used_indices[\"source_file\"] == fname, \"index\"].tolist()\n",
    "        df_avail = df.loc[~df.index.isin(used_idx_this_file)]\n",
    "\n",
    "        if df_avail.empty:\n",
    "            print(f\"âš ï¸ {fname} å¯ç”¨æ•°æ®ä¸º 0ï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "\n",
    "        # æœ¬æ–‡ä»¶å¯æŠ½å–æ•°é‡\n",
    "        take_n = min(remain, len(df_avail))\n",
    "\n",
    "        sampled_df = df_avail.sample(n=take_n, random_state=42)\n",
    "\n",
    "        # å†™å…¥è¾“å‡ºæ–‡ä»¶\n",
    "        sampled_df.to_csv(save_path, mode=\"a\", header=first_write, index=False)\n",
    "        first_write = False\n",
    "\n",
    "        # æ›´æ–° used_indices\n",
    "        new_used = pd.DataFrame({\n",
    "            \"source_file\": fname,\n",
    "            \"index\": sampled_df.index,\n",
    "            \"label\": label\n",
    "        })\n",
    "        used_indices = pd.concat([used_indices, new_used], ignore_index=True)\n",
    "\n",
    "        print(f\"ğŸ“¦ ä» {fname} æŠ½å– {take_n} æ¡ã€‚\")\n",
    "\n",
    "        remain -= take_n\n",
    "        if remain <= 0:\n",
    "            print(f\"ğŸ‰ ç±»åˆ« {label} å·²æŠ½æ»¡ {target_n} æ¡ã€‚\")\n",
    "            break\n",
    "\n",
    "# ===================== 6ï¸âƒ£ ä¿å­˜ä½¿ç”¨è®°å½• =====================\n",
    "used_indices.to_csv(used_index_path, index=False)\n",
    "print(\"\\n==============================\")\n",
    "print(f\"ğŸ æ‰€æœ‰ç±»åˆ«æŠ½æ ·å®Œæˆï¼\")\n",
    "print(f\"ğŸ“ å·²ä¿å­˜ï¼š{save_path}\")\n",
    "print(f\"ğŸ“ ä½¿ç”¨è®°å½•ï¼š{used_index_path}\")\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823e6dc",
   "metadata": {},
   "source": [
    "### è®­ç»ƒé›†ã€æµ‹è¯•é›†ã€éªŒè¯é›†åˆ’åˆ†ï¼Œå¦å­˜ä¸ºæ–°çš„csvæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47291c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ•°æ®ï¼š /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/mixed_allclass.csv\n",
      "æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± 70000 æ¡è®°å½•\n",
      "ç§»é™¤æœ€åä¸€åˆ—ï¼šsource_file\n",
      "æ‰€æœ‰æ•°æ®é›†å·²ä¿å­˜ï¼ˆä¿æŒåŸé¡ºåºï¼Œå·²å»é™¤æœ€åä¸€åˆ—ï¼‰ã€‚\n",
      "æ—¥å¿—å†™å…¥å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============= 1. è·¯å¾„è®¾ç½® =============\n",
    "root_dir = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode\"\n",
    "csv_path = os.path.join(root_dir, \"mixed_allclass.csv\")\n",
    "\n",
    "print(\"åŠ è½½æ•°æ®ï¼š\", csv_path)\n",
    "\n",
    "train_dir = os.path.join(root_dir, \"train\")\n",
    "val_dir = os.path.join(root_dir, \"val\")\n",
    "test_dir = os.path.join(root_dir, \"test\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# ============= 2. è¯»å–æ•°æ® =============\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•\")\n",
    "\n",
    "# è®°å½•åŸå§‹ç´¢å¼•é¡ºåº\n",
    "original_index = df.index.to_numpy()\n",
    "\n",
    "# ============= 3. è®¾ç½®åˆ’åˆ†æ¯”ä¾‹ =============\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "total = len(df)\n",
    "\n",
    "# ============= 4. éšæœºæŠ½å–ç´¢å¼•ï¼Œä¸æ‰“ä¹±é¡ºåº =============\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# éšæœºé‡‡æ ·ï¼Œä½†ä¸æ”¹å˜åŸå§‹é¡ºåº\n",
    "all_indices = np.arange(total)\n",
    "rng.shuffle(all_indices)\n",
    "\n",
    "train_end = int(total * train_ratio)\n",
    "val_end = train_end + int(total * val_ratio)\n",
    "\n",
    "train_idx = np.sort(all_indices[:train_end])\n",
    "val_idx = np.sort(all_indices[train_end:val_end])\n",
    "test_idx = np.sort(all_indices[val_end:])\n",
    "\n",
    "# ============= 5. ä¿æŒåŸé¡ºåºæŠ½å–æ•°æ® =============\n",
    "train_df = df.loc[train_idx].copy()\n",
    "val_df = df.loc[val_idx].copy()\n",
    "test_df = df.loc[test_idx].copy()\n",
    "\n",
    "# ============= 6. åˆ é™¤æœ€åä¸€åˆ—ï¼ˆsource_fileï¼‰ =============\n",
    "last_col = df.columns[-1]\n",
    "print(f\"ç§»é™¤æœ€åä¸€åˆ—ï¼š{last_col}\")\n",
    "\n",
    "train_df = train_df.iloc[:, :-1]\n",
    "val_df = val_df.iloc[:, :-1]\n",
    "test_df = test_df.iloc[:, :-1]\n",
    "\n",
    "# ============= 7. ä¿å­˜ CSV =============\n",
    "train_df.to_csv(os.path.join(train_dir, \"train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(val_dir, \"val.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(test_dir, \"test.csv\"), index=False)\n",
    "\n",
    "print(\"æ‰€æœ‰æ•°æ®é›†å·²ä¿å­˜ï¼ˆä¿æŒåŸé¡ºåºï¼Œå·²å»é™¤æœ€åä¸€åˆ—ï¼‰ã€‚\")\n",
    "\n",
    "# ============= 8. ä¿å­˜æ—¥å¿— =============\n",
    "log_path = os.path.join(root_dir, \"split_log.txt\")\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(\"KAIST all_class æ•°æ®é›†éšæœºåˆ’åˆ†æ—¥å¿—ï¼ˆä¿æŒåŸé¡ºåº+å¿½ç•¥æœ€åä¸€åˆ—ï¼‰\\n\\n\")\n",
    "    f.write(f\"æ€»æ ·æœ¬æ•°: {len(df)}\\n\")\n",
    "    f.write(f\"Train: {len(train_df)}\\n\")\n",
    "    f.write(f\"Val:   {len(val_df)}\\n\")\n",
    "    f.write(f\"Test:  {len(test_df)}\\n\\n\")\n",
    "    f.write(\"åˆ’åˆ†æ¯”ä¾‹:\\n\")\n",
    "    f.write(f\"train_ratio = {train_ratio}\\n\")\n",
    "    f.write(f\"val_ratio   = {val_ratio}\\n\")\n",
    "    f.write(f\"test_ratio  = {test_ratio}\\n\")\n",
    "\n",
    "print(\"æ—¥å¿—å†™å…¥å®Œæˆã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801c064",
   "metadata": {},
   "source": [
    "### mixed_data->pyg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47219ccb",
   "metadata": {},
   "source": [
    "#### æ—¶é—´é‚»ç‚¹å»ºå›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8b4744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/data_process/KAIST\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/data_process/KAIST/my_lib\n",
      "\n",
      "ğŸš€ å¼€å§‹æ„å»º train å›¾ç»“æ„...\n",
      "ğŸ“ pyg ä¿å­˜ç›®å½•: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg\n",
      "âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± 42000 ä¸ªèŠ‚ç‚¹ï¼Œ4200000 æ¡æ— å‘è¾¹\n",
      "ğŸ“ nodes.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/nodes.csv\n",
      "ğŸ“ edges.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/edges.csv\n",
      "ğŸ“ graph.pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/graph.pt\n",
      "ğŸ§© ç‰¹å¾ç»´åº¦: 4 (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)\n",
      "   ğŸ“„ nodes_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/nodes.csv\n",
      "   ğŸ“„ edges_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/edges.csv\n",
      "   ğŸ“¦ graph_pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/graph.pt\n",
      "âœ… train å›¾æ„å»ºå®Œæˆã€‚\n",
      "\n",
      "ğŸš€ å¼€å§‹æ„å»º val å›¾ç»“æ„...\n",
      "ğŸ“ pyg ä¿å­˜ç›®å½•: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/val/pyg\n",
      "âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± 14000 ä¸ªèŠ‚ç‚¹ï¼Œ1400000 æ¡æ— å‘è¾¹\n",
      "ğŸ“ nodes.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/val/pyg/nodes.csv\n",
      "ğŸ“ edges.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/val/pyg/edges.csv\n",
      "ğŸ“ graph.pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/val/pyg/graph.pt\n",
      "ğŸ§© ç‰¹å¾ç»´åº¦: 4 (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)\n",
      "   ğŸ“„ nodes_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/val/pyg/nodes.csv\n",
      "   ğŸ“„ edges_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/val/pyg/edges.csv\n",
      "   ğŸ“¦ graph_pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/val/pyg/graph.pt\n",
      "âœ… val å›¾æ„å»ºå®Œæˆã€‚\n",
      "\n",
      "ğŸš€ å¼€å§‹æ„å»º test å›¾ç»“æ„...\n",
      "ğŸ“ pyg ä¿å­˜ç›®å½•: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/test/pyg\n",
      "âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± 14000 ä¸ªèŠ‚ç‚¹ï¼Œ1400000 æ¡æ— å‘è¾¹\n",
      "ğŸ“ nodes.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/test/pyg/nodes.csv\n",
      "ğŸ“ edges.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/test/pyg/edges.csv\n",
      "ğŸ“ graph.pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/test/pyg/graph.pt\n",
      "ğŸ§© ç‰¹å¾ç»´åº¦: 4 (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)\n",
      "   ğŸ“„ nodes_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/test/pyg/nodes.csv\n",
      "   ğŸ“„ edges_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/test/pyg/edges.csv\n",
      "   ğŸ“¦ graph_pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/test/pyg/graph.pt\n",
      "âœ… test å›¾æ„å»ºå®Œæˆã€‚\n",
      "\n",
      "ğŸ‰ ä¸‰ä¸ªæ•°æ®é›† (train/val/test) çš„å›¾å·²ç»å…¨éƒ¨ç‹¬ç«‹æ„å»ºå®Œæ¯•ï¼\n"
     ]
    }
   ],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "from build_pyg_data import build_local_temporal_graph   # â˜… ä½ è¦ç”¨çš„å‡½æ•°\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# ================== 2ï¸âƒ£ æ ¹ç›®å½• ==================\n",
    "root_dir = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode\"\n",
    "\n",
    "splits = {\n",
    "    \"train\": os.path.join(root_dir, \"train/train.csv\"),\n",
    "    \"val\":   os.path.join(root_dir, \"val/val.csv\"),\n",
    "    \"test\":  os.path.join(root_dir, \"test/test.csv\"),\n",
    "}\n",
    "\n",
    "# ================== 3ï¸âƒ£ å»ºå›¾å‚æ•°å¯è°ƒ ==================\n",
    "num_edges = 100     # æ—¶é—´ç›¸é‚» KNN æ•°ï¼ˆä½ è‡ªå·±å®šï¼‰\n",
    "\n",
    "# ================== 4ï¸âƒ£ åˆ†åˆ«ä¸ºä¸‰ä¸ªé›†å»ºå›¾ ==================\n",
    "for split_name, csv_path in splits.items():\n",
    "\n",
    "    print(f\"\\nğŸš€ å¼€å§‹æ„å»º {split_name} å›¾ç»“æ„...\")\n",
    "    assert os.path.exists(csv_path), f\"âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨ï¼š{csv_path}\"\n",
    "\n",
    "    # === ä¿å­˜è·¯å¾„ï¼štrain/pyg/ã€val/pyg/ã€test/pyg/ ===\n",
    "    pyg_dir = os.path.join(os.path.dirname(csv_path), \"pyg\")\n",
    "    os.makedirs(pyg_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"ğŸ“ pyg ä¿å­˜ç›®å½•: {pyg_dir}\")\n",
    "\n",
    "    # === è°ƒç”¨æœ¬åœ°æ—¶åºå»ºå›¾å‡½æ•° ===\n",
    "    nodes_csv, edges_csv, graph_pt = build_local_temporal_graph(\n",
    "        csv_path=csv_path,\n",
    "        save_dir=pyg_dir,\n",
    "        num_edges=num_edges\n",
    "    )\n",
    "\n",
    "    print(f\"   ğŸ“„ nodes_csv: {nodes_csv}\")\n",
    "    print(f\"   ğŸ“„ edges_csv: {edges_csv}\")\n",
    "    print(f\"   ğŸ“¦ graph_pt : {graph_pt}\")\n",
    "\n",
    "    print(f\"âœ… {split_name} å›¾æ„å»ºå®Œæˆã€‚\")\n",
    "\n",
    "print(\"\\nğŸ‰ ä¸‰ä¸ªæ•°æ®é›† (train/val/test) çš„å›¾å·²ç»å…¨éƒ¨ç‹¬ç«‹æ„å»ºå®Œæ¯•ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c5a41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/data_process/KAIST\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/data_process/KAIST/my_lib\n",
      "ğŸ“Œ åŠ è½½ train å›¾ï¼š/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/graph.pt\n",
      "èŠ‚ç‚¹æ•° = 42000\n",
      "ğŸ‘‰ æœ‰æ ‡ç­¾èŠ‚ç‚¹ï¼š12600  | æ— æ ‡ç­¾èŠ‚ç‚¹ï¼š29400\n",
      "âœ… æ©ç æ·»åŠ å®Œæˆï¼š\n",
      "  train_withlabel_mask.sum() = 12600\n",
      "  train_nolabel_mask.sum()   = 29400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_798073/3034512645.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(train_graph_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ æ–°å›¾å·²ä¿å­˜ä¸ºï¼š/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/train_graph_with_labelmask.pt\n"
     ]
    }
   ],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ================== 2ï¸âƒ£ è¯»å– train å›¾ ==================\n",
    "train_graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode/train/pyg/graph.pt\"\n",
    "assert os.path.exists(train_graph_path), f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{train_graph_path}\"\n",
    "\n",
    "print(f\"ğŸ“Œ åŠ è½½ train å›¾ï¼š{train_graph_path}\")\n",
    "data = torch.load(train_graph_path)\n",
    "\n",
    "total_nodes = data.x.size(0)\n",
    "print(f\"èŠ‚ç‚¹æ•° = {total_nodes}\")\n",
    "\n",
    "# ================== 3ï¸âƒ£ è®¾ç½®æ¯”ä¾‹ ==================\n",
    "train_label_ratio = 0.3   # 30% èŠ‚ç‚¹ä¸º train_withlabel\n",
    "\n",
    "num_withlabel = int(total_nodes * train_label_ratio)\n",
    "num_nolabel = total_nodes - num_withlabel\n",
    "\n",
    "print(f\"ğŸ‘‰ æœ‰æ ‡ç­¾èŠ‚ç‚¹ï¼š{num_withlabel}  | æ— æ ‡ç­¾èŠ‚ç‚¹ï¼š{num_nolabel}\")\n",
    "\n",
    "# ================== 4ï¸âƒ£ éšæœºç”Ÿæˆ maskï¼ˆä¿æŒå¯å¤ç°ï¼‰ ==================\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "all_indices = np.arange(total_nodes)\n",
    "rng.shuffle(all_indices)\n",
    "\n",
    "withlabel_idx = all_indices[:num_withlabel]\n",
    "nolabel_idx   = all_indices[num_withlabel:]\n",
    "\n",
    "# åˆ›å»º mask\n",
    "train_withlabel_mask = np.zeros(total_nodes, dtype=bool)\n",
    "train_nolabel_mask   = np.zeros(total_nodes, dtype=bool)\n",
    "\n",
    "train_withlabel_mask[withlabel_idx] = True\n",
    "train_nolabel_mask[nolabel_idx] = True\n",
    "\n",
    "# ================== 5ï¸âƒ£ æ·»åŠ åˆ° Data å¯¹è±¡ ==================\n",
    "data.train_withlabel_mask = torch.tensor(train_withlabel_mask)\n",
    "data.train_nolabel_mask   = torch.tensor(train_nolabel_mask)\n",
    "\n",
    "print(\"âœ… æ©ç æ·»åŠ å®Œæˆï¼š\")\n",
    "print(f\"  train_withlabel_mask.sum() = {int(data.train_withlabel_mask.sum())}\")\n",
    "print(f\"  train_nolabel_mask.sum()   = {int(data.train_nolabel_mask.sum())}\")\n",
    "\n",
    "# ================== 6ï¸âƒ£ ä¿å­˜æ–°çš„å›¾ç»“æ„ ==================\n",
    "save_path = os.path.join(\n",
    "    os.path.dirname(train_graph_path),\n",
    "    \"train_graph_with_labelmask.pt\"\n",
    ")\n",
    "\n",
    "torch.save(data, save_path)\n",
    "print(f\"ğŸ‰ æ–°å›¾å·²ä¿å­˜ä¸ºï¼š{save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51403d82",
   "metadata": {},
   "source": [
    "#### é«˜ç›¸ä¼¼åº¦ç±»åˆ«çš„å»ºå›¾,æ»‘åŠ¨çª—å£å»ºå›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073089c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ åŸå§‹æ•°æ®é‡: 22700\n",
      "ğŸ“Œ ç­›é€‰åæ•°æ®é‡: 7500\n",
      "âœ… å·²ä¿å­˜ç­›é€‰åçš„æ•°æ®ï¼š/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/mixed_high_similar.csv\n",
      "ğŸ“Œ Train: 4500, Val: 1500, Test: 1500\n",
      "\n",
      "âœ… Train/Val/Test å·²ä¿å­˜ï¼ˆéšæœºåˆ’åˆ†ï¼Œä½†é¡ºåºä¿æŒä¸å˜ï¼‰ï¼š\n",
      "   ğŸŸ© Train: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/train.csv\n",
      "   ğŸŸ¦ Val  : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/val.csv\n",
      "   ğŸŸ¥ Test : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/test.csv\n",
      "\n",
      "ğŸ‰ éšæœºåˆ’åˆ† + é¡ºåºä¿å­˜ 6:2:2 å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥åº“ ==================\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ================== 2ï¸âƒ£ åŸå§‹ CSV è·¯å¾„ ==================\n",
    "input_csv = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/mixed_allclass/mixed_allclass_nofinal.csv\"\n",
    "assert os.path.exists(input_csv), f\"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_csv}\"\n",
    "\n",
    "# ================== 3ï¸âƒ£ è¾“å‡ºæ ¹ç›®å½• ==================\n",
    "output_root = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "# ================== 4ï¸âƒ£ è¯»å–æ•°æ® ==================\n",
    "df = pd.read_csv(input_csv)\n",
    "print(f\"ğŸ“Œ åŸå§‹æ•°æ®é‡: {len(df)}\")\n",
    "\n",
    "# ================== 5ï¸âƒ£ ç­›é€‰ label {1,11,12,13,14} ==================\n",
    "target_labels = [1, 11, 12, 13, 14]\n",
    "df_filtered = df[df[\"label\"].isin(target_labels)]\n",
    "print(f\"ğŸ“Œ ç­›é€‰åæ•°æ®é‡: {len(df_filtered)}\")\n",
    "\n",
    "# ä¿å­˜ç­›é€‰åçš„æ•°æ®\n",
    "filtered_csv_path = os.path.join(output_root, \"mixed_high_similar.csv\")\n",
    "df_filtered.to_csv(filtered_csv_path, index=False)\n",
    "print(f\"âœ… å·²ä¿å­˜ç­›é€‰åçš„æ•°æ®ï¼š{filtered_csv_path}\")\n",
    "\n",
    "# ================== 6ï¸âƒ£ éšæœºåˆ’åˆ† (6 : 2 : 2)ï¼Œä½†ä¿æŒç´¢å¼• ==================\n",
    "# step1: å…ˆå–å¾—æ‰€æœ‰ indexï¼ˆä¸æ‰“ä¹±åŸ df_filtered ç§©åºï¼‰\n",
    "indices = df_filtered.index.to_numpy()\n",
    "\n",
    "# step2: éšæœºåˆ’åˆ† indicesï¼ˆçœŸæ­£çš„éšæœºæ€§åœ¨è¿™é‡Œï¼‰\n",
    "train_idx, temp_idx = train_test_split(\n",
    "    indices, test_size=0.4, random_state=42\n",
    ")\n",
    "val_idx, test_idx = train_test_split(\n",
    "    temp_idx, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# step3: ç”¨ iloc æ ¹æ® index é€‰æ•°æ®ï¼Œä½†ä¸æ‰“ä¹±ï¼Œä¿æŒåŸ df é¡ºåº\n",
    "train_df = df_filtered.loc[sorted(train_idx)]\n",
    "val_df   = df_filtered.loc[sorted(val_idx)]\n",
    "test_df  = df_filtered.loc[sorted(test_idx)]\n",
    "\n",
    "print(f\"ğŸ“Œ Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# ================== 7ï¸âƒ£ ä¿å­˜ ==================\n",
    "train_dir = os.path.join(output_root, \"train\")\n",
    "val_dir   = os.path.join(output_root, \"val\")\n",
    "test_dir  = os.path.join(output_root, \"test\")\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(os.path.join(train_dir, \"train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(val_dir, \"val.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(test_dir, \"test.csv\"), index=False)\n",
    "\n",
    "print(\"\\nâœ… Train/Val/Test å·²ä¿å­˜ï¼ˆéšæœºåˆ’åˆ†ï¼Œä½†é¡ºåºä¿æŒä¸å˜ï¼‰ï¼š\")\n",
    "print(f\"   ğŸŸ© Train: {os.path.join(train_dir, 'train.csv')}\")\n",
    "print(f\"   ğŸŸ¦ Val  : {os.path.join(val_dir, 'val.csv')}\")\n",
    "print(f\"   ğŸŸ¥ Test : {os.path.join(test_dir, 'test.csv')}\")\n",
    "\n",
    "print(\"\\nğŸ‰ éšæœºåˆ’åˆ† + é¡ºåºä¿å­˜ 6:2:2 å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e42a5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/data_process/KAIST\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/data_process/KAIST/my_lib\n",
      "\n",
      "ğŸš€ å¼€å§‹æ„å»º train å›¾ç»“æ„...\n",
      "ğŸ“ pyg ä¿å­˜ç›®å½•: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg\n",
      "âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± 4500 ä¸ªèŠ‚ç‚¹ï¼Œ4500000 æ¡æ— å‘è¾¹\n",
      "ğŸ“ nodes.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/nodes.csv\n",
      "ğŸ“ edges.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/edges.csv\n",
      "ğŸ“ graph.pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/graph.pt\n",
      "ğŸ§© ç‰¹å¾ç»´åº¦: 4 (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)\n",
      "   ğŸ“„ nodes_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/nodes.csv\n",
      "   ğŸ“„ edges_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/edges.csv\n",
      "   ğŸ“¦ graph_pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/graph.pt\n",
      "âœ… train å›¾æ„å»ºå®Œæˆã€‚\n",
      "\n",
      "ğŸš€ å¼€å§‹æ„å»º val å›¾ç»“æ„...\n",
      "ğŸ“ pyg ä¿å­˜ç›®å½•: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg\n",
      "âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± 1500 ä¸ªèŠ‚ç‚¹ï¼Œ1500000 æ¡æ— å‘è¾¹\n",
      "ğŸ“ nodes.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg/nodes.csv\n",
      "ğŸ“ edges.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg/edges.csv\n",
      "ğŸ“ graph.pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg/graph.pt\n",
      "ğŸ§© ç‰¹å¾ç»´åº¦: 4 (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)\n",
      "   ğŸ“„ nodes_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg/nodes.csv\n",
      "   ğŸ“„ edges_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg/edges.csv\n",
      "   ğŸ“¦ graph_pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg/graph.pt\n",
      "âœ… val å›¾æ„å»ºå®Œæˆã€‚\n",
      "\n",
      "ğŸš€ å¼€å§‹æ„å»º test å›¾ç»“æ„...\n",
      "ğŸ“ pyg ä¿å­˜ç›®å½•: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg\n",
      "âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± 1500 ä¸ªèŠ‚ç‚¹ï¼Œ1500000 æ¡æ— å‘è¾¹\n",
      "ğŸ“ nodes.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg/nodes.csv\n",
      "ğŸ“ edges.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg/edges.csv\n",
      "ğŸ“ graph.pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg/graph.pt\n",
      "ğŸ§© ç‰¹å¾ç»´åº¦: 4 (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)\n",
      "   ğŸ“„ nodes_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg/nodes.csv\n",
      "   ğŸ“„ edges_csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg/edges.csv\n",
      "   ğŸ“¦ graph_pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg/graph.pt\n",
      "âœ… test å›¾æ„å»ºå®Œæˆã€‚\n",
      "\n",
      "ğŸ‰ ä¸‰ä¸ªæ•°æ®é›† (train/val/test) çš„å›¾å·²ç»å…¨éƒ¨ç‹¬ç«‹æ„å»ºå®Œæ¯•ï¼\n"
     ]
    }
   ],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "from build_pyg_data import build_local_temporal_graph   # â˜… ä½ è¦ç”¨çš„å‡½æ•°\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# ================== 2ï¸âƒ£ æ ¹ç›®å½• ==================\n",
    "root_dir = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar\"\n",
    "\n",
    "splits = {\n",
    "    \"train\": os.path.join(root_dir, \"train/train.csv\"),\n",
    "    \"val\":   os.path.join(root_dir, \"val/val.csv\"),\n",
    "    \"test\":  os.path.join(root_dir, \"test/test.csv\"),\n",
    "}\n",
    "\n",
    "# ================== 3ï¸âƒ£ å»ºå›¾å‚æ•°å¯è°ƒ ==================\n",
    "num_edges = 1000     # æ—¶é—´ç›¸é‚» KNN æ•°ï¼ˆä½ è‡ªå·±å®šï¼‰\n",
    "\n",
    "# ================== 4ï¸âƒ£ åˆ†åˆ«ä¸ºä¸‰ä¸ªé›†å»ºå›¾ ==================\n",
    "for split_name, csv_path in splits.items():\n",
    "\n",
    "    print(f\"\\nğŸš€ å¼€å§‹æ„å»º {split_name} å›¾ç»“æ„...\")\n",
    "    assert os.path.exists(csv_path), f\"âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨ï¼š{csv_path}\"\n",
    "\n",
    "    # === ä¿å­˜è·¯å¾„ï¼štrain/pyg/ã€val/pyg/ã€test/pyg/ ===\n",
    "    pyg_dir = os.path.join(os.path.dirname(csv_path), \"pyg\")\n",
    "    os.makedirs(pyg_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"ğŸ“ pyg ä¿å­˜ç›®å½•: {pyg_dir}\")\n",
    "\n",
    "    # === è°ƒç”¨æœ¬åœ°æ—¶åºå»ºå›¾å‡½æ•° ===\n",
    "    nodes_csv, edges_csv, graph_pt = build_local_temporal_graph(\n",
    "        csv_path=csv_path,\n",
    "        save_dir=pyg_dir,\n",
    "        num_edges=num_edges\n",
    "    )\n",
    "\n",
    "    print(f\"   ğŸ“„ nodes_csv: {nodes_csv}\")\n",
    "    print(f\"   ğŸ“„ edges_csv: {edges_csv}\")\n",
    "    print(f\"   ğŸ“¦ graph_pt : {graph_pt}\")\n",
    "\n",
    "    print(f\"âœ… {split_name} å›¾æ„å»ºå®Œæˆã€‚\")\n",
    "\n",
    "print(\"\\nğŸ‰ ä¸‰ä¸ªæ•°æ®é›† (train/val/test) çš„å›¾å·²ç»å…¨éƒ¨ç‹¬ç«‹æ„å»ºå®Œæ¯•ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec93b43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/data_process/KAIST\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/data_process/KAIST/my_lib\n",
      "ğŸ“Œ åŠ è½½ train å›¾ï¼š/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/graph.pt\n",
      "èŠ‚ç‚¹æ•° = 4500\n",
      "ğŸ‘‰ æœ‰æ ‡ç­¾èŠ‚ç‚¹ï¼š1350  | æ— æ ‡ç­¾èŠ‚ç‚¹ï¼š3150\n",
      "âœ… æ©ç æ·»åŠ å®Œæˆï¼š\n",
      "  train_withlabel_mask.sum() = 1350\n",
      "  train_nolabel_mask.sum()   = 3150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32966/3477331405.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(train_graph_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ æ–°å›¾å·²ä¿å­˜ä¸ºï¼š/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/train_graph_with_labelmask.pt\n"
     ]
    }
   ],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ================== 2ï¸âƒ£ è¯»å– train å›¾ ==================\n",
    "train_graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/graph.pt\"\n",
    "assert os.path.exists(train_graph_path), f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{train_graph_path}\"\n",
    "\n",
    "print(f\"ğŸ“Œ åŠ è½½ train å›¾ï¼š{train_graph_path}\")\n",
    "data = torch.load(train_graph_path)\n",
    "\n",
    "total_nodes = data.x.size(0)\n",
    "print(f\"èŠ‚ç‚¹æ•° = {total_nodes}\")\n",
    "\n",
    "# ================== 3ï¸âƒ£ è®¾ç½®æ¯”ä¾‹ ==================\n",
    "train_label_ratio = 0.3   # 30% èŠ‚ç‚¹ä¸º train_withlabel\n",
    "\n",
    "num_withlabel = int(total_nodes * train_label_ratio)\n",
    "num_nolabel = total_nodes - num_withlabel\n",
    "\n",
    "print(f\"ğŸ‘‰ æœ‰æ ‡ç­¾èŠ‚ç‚¹ï¼š{num_withlabel}  | æ— æ ‡ç­¾èŠ‚ç‚¹ï¼š{num_nolabel}\")\n",
    "\n",
    "# ================== 4ï¸âƒ£ éšæœºç”Ÿæˆ maskï¼ˆä¿æŒå¯å¤ç°ï¼‰ ==================\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "all_indices = np.arange(total_nodes)\n",
    "rng.shuffle(all_indices)\n",
    "\n",
    "withlabel_idx = all_indices[:num_withlabel]\n",
    "nolabel_idx   = all_indices[num_withlabel:]\n",
    "\n",
    "# åˆ›å»º mask\n",
    "train_withlabel_mask = np.zeros(total_nodes, dtype=bool)\n",
    "train_nolabel_mask   = np.zeros(total_nodes, dtype=bool)\n",
    "\n",
    "train_withlabel_mask[withlabel_idx] = True\n",
    "train_nolabel_mask[nolabel_idx] = True\n",
    "\n",
    "# ================== 5ï¸âƒ£ æ·»åŠ åˆ° Data å¯¹è±¡ ==================\n",
    "data.train_withlabel_mask = torch.tensor(train_withlabel_mask)\n",
    "data.train_nolabel_mask   = torch.tensor(train_nolabel_mask)\n",
    "\n",
    "print(\"âœ… æ©ç æ·»åŠ å®Œæˆï¼š\")\n",
    "print(f\"  train_withlabel_mask.sum() = {int(data.train_withlabel_mask.sum())}\")\n",
    "print(f\"  train_nolabel_mask.sum()   = {int(data.train_nolabel_mask.sum())}\")\n",
    "\n",
    "# ================== 6ï¸âƒ£ ä¿å­˜æ–°çš„å›¾ç»“æ„ ==================\n",
    "save_path = os.path.join(\n",
    "    os.path.dirname(train_graph_path),\n",
    "    \"train_graph_with_labelmask.pt\"\n",
    ")\n",
    "\n",
    "torch.save(data, save_path)\n",
    "print(f\"ğŸ‰ æ–°å›¾å·²ä¿å­˜ä¸ºï¼š{save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86614f61",
   "metadata": {},
   "source": [
    "#### ä½™å¼¦ç›¸ä¼¼åº¦å»ºå›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "# %run ../_init_path.py\n",
    "# from build_pyg_data import build_similarity_knn_graph\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# # ================== 2ï¸âƒ£ å‚æ•°è®¾ç½® ==================\n",
    "# csv_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/mixed_allclass/mixed_allclass_nofinal.csv\"\n",
    "# save_dir = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/mixed_allclass/pyg_data/cos_knn\"\n",
    "\n",
    "# # ================== 3ï¸âƒ£ è°ƒç”¨å»ºå›¾å‡½æ•° ==================\n",
    "# print(\"ğŸš€ å¼€å§‹æ„å»ºåŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„KNNå›¾ ...\")\n",
    "# nodes_csv, edges_csv, graph_pt = build_similarity_knn_graph(\n",
    "#     csv_path=csv_path,\n",
    "#     save_dir=save_dir,\n",
    "#     num_edges=10\n",
    "# )\n",
    "# print(\"\\nâœ… å›¾ç»“æ„å·²ç”Ÿæˆï¼š\")\n",
    "# print(f\"ğŸ“ èŠ‚ç‚¹æ–‡ä»¶: {nodes_csv}\")\n",
    "# print(f\"ğŸ“ è¾¹æ–‡ä»¶:   {edges_csv}\")\n",
    "# print(f\"ğŸ“ å›¾æ–‡ä»¶:   {graph_pt}\")\n",
    "\n",
    "# # ================== 4ï¸âƒ£ éªŒè¯ç”Ÿæˆçš„å›¾æ•°æ® ==================\n",
    "# print(\"\\nğŸ§© éªŒè¯ç”Ÿæˆçš„ PyG å›¾æ•°æ®ç»“æ„ ...\")\n",
    "\n",
    "# # è¯»å– PyG å¯¹è±¡\n",
    "# data = torch.load(graph_pt, weights_only=False)\n",
    "\n",
    "# # === åŸºç¡€ä¿¡æ¯ ===\n",
    "# num_nodes = data.x.size(0)\n",
    "# num_edges = data.edge_index.size(1)\n",
    "# num_features = data.x.size(1)\n",
    "# has_label = hasattr(data, \"y\") and data.y is not None\n",
    "\n",
    "# print(f\"ğŸ“¦ Data å¯¹è±¡ç»“æ„: {data}\")\n",
    "# print(f\"ğŸ§® èŠ‚ç‚¹æ•°: {num_nodes}\")\n",
    "# print(f\"ğŸ§® è¾¹æ•°: {num_edges // 2}ï¼ˆæ— å‘è¾¹ï¼‰\")\n",
    "# print(f\"ğŸ§© ç‰¹å¾ç»´åº¦: {num_features}\")\n",
    "# print(f\"ğŸ·ï¸  æ ‡ç­¾å­˜åœ¨: {'âœ… æ˜¯' if has_label else 'âŒ å¦'}\")\n",
    "\n",
    "# # === æ ‡ç­¾éªŒè¯ ===\n",
    "# if has_label:\n",
    "#     y = data.y.cpu().numpy()\n",
    "#     unique_labels = sorted(set(y))\n",
    "#     print(f\"ğŸ¯ æ ‡ç­¾ç±»åˆ«æ•°é‡: {len(unique_labels)} | ç±»åˆ«: {unique_labels[:10]}\")\n",
    "#     print(f\"å‰5ä¸ªæ ‡ç­¾: {y[:5]}\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ æœªæ£€æµ‹åˆ° data.yï¼Œæ ‡ç­¾åˆ—å¯èƒ½ç¼ºå¤±æˆ–æœªæ­£ç¡®æŒ‡å®šã€‚\")\n",
    "\n",
    "# # === èŠ‚ç‚¹/è¾¹æ–‡ä»¶éªŒè¯ ===\n",
    "# print(\"\\nğŸ“Š éªŒè¯èŠ‚ç‚¹å’Œè¾¹ CSV æ–‡ä»¶ ...\")\n",
    "# df_nodes = pd.read_csv(nodes_csv)\n",
    "# df_edges = pd.read_csv(edges_csv)\n",
    "\n",
    "# print(f\"âœ… èŠ‚ç‚¹æ–‡ä»¶è¡Œæ•°: {len(df_nodes)} | ä¸å›¾èŠ‚ç‚¹æ•° {'ä¸€è‡´ âœ…' if len(df_nodes)==num_nodes else 'ä¸ä¸€è‡´ âš ï¸'}\")\n",
    "# print(f\"âœ… è¾¹æ–‡ä»¶è¡Œæ•°: {len(df_edges)} | æ¯ä¸¤è¡Œå¯¹åº”ä¸€æ¡æ— å‘è¾¹\")\n",
    "\n",
    "# if \"source\" in df_edges.columns and \"target\" in df_edges.columns:\n",
    "#     unique_nodes = pd.concat([df_edges[\"source\"], df_edges[\"target\"]]).nunique()\n",
    "#     print(f\"ğŸ§® è¾¹ä¸­æ¶‰åŠçš„å”¯ä¸€èŠ‚ç‚¹æ•°: {unique_nodes}\")\n",
    "# else:\n",
    "#     print(\"âš ï¸ è¾¹æ–‡ä»¶ç¼ºå°‘ 'source' æˆ– 'target' åˆ—ï¼\")\n",
    "\n",
    "# print(\"\\nğŸ¯ å›¾æ•°æ®éªŒè¯å®Œæˆã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "# %run ../_init_path.py\n",
    "# from build_pyg_data import add_random_masks_with_label_split\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# # ================== 2ï¸âƒ£ å‚æ•°è®¾ç½® ==================\n",
    "# ratios = {\"train\": 0.6, \"val\": 0.2, \"test\": 0.2}\n",
    "\n",
    "# graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/mixed_allclass/pyg_data/cos_knn/mixed_allclass_nofinal_graph.pt\"\n",
    "# save_path  = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/mixed_allclass/pyg_data/cos_knn/mixed_allclass_arabic_graph_withmask.pt\"\n",
    "\n",
    "# # ================== 3ï¸âƒ£ ç”Ÿæˆæ©ç  ==================\n",
    "# print(\"ğŸš€ å¼€å§‹ä¸ºå›¾æ·»åŠ éšæœºæ©ç  ...\")\n",
    "# new_graph_path = add_random_masks_with_label_split(\n",
    "#     graph_path=graph_path,\n",
    "#     save_path=save_path,     # è‹¥æƒ³è¦†ç›–æºæ–‡ä»¶åˆ™ç›´æ¥ä¼  None\n",
    "#     ratios=ratios,\n",
    "#     train_label_ratio=0.4,   # è®­ç»ƒé›†ä¸­40%ä¸ºæœ‰æ ‡ç­¾\n",
    "# )\n",
    "# print(f\"\\nâœ… æ©ç ç”Ÿæˆä¸ä¿å­˜å®Œæˆï¼š{new_graph_path}\")\n",
    "\n",
    "# # ================== 4ï¸âƒ£ éªŒè¯ç”Ÿæˆç»“æœ ==================\n",
    "# print(\"\\nğŸ§© éªŒè¯ç”Ÿæˆçš„å›¾ç»“æ„ä¸æ©ç  ...\")\n",
    "\n",
    "# # è¯»å– PyG æ•°æ®\n",
    "# data = torch.load(new_graph_path, weights_only=False)\n",
    "\n",
    "# # === åŸºæœ¬ä¿¡æ¯ ===\n",
    "# print(f\"\\nğŸ“¦ å›¾æ•°æ®ç»“æ„: {data}\")\n",
    "# print(f\"x shape: {data.x.shape}, y shape: {data.y.shape}\")\n",
    "# print(f\"å‰5ä¸ªæ ‡ç­¾: {data.y[:5].tolist()}\")\n",
    "\n",
    "# # === æ©ç å­˜åœ¨æ€§æ£€æµ‹ ===\n",
    "# mask_attrs = [\"train_mask\", \"val_mask\", \"test_mask\", \"train_withlabel_mask\", \"train_nolabel_mask\"]\n",
    "# print(\"\\nğŸ“‹ æ©ç å­˜åœ¨æ€§æ£€æŸ¥ï¼š\")\n",
    "# for attr in mask_attrs:\n",
    "#     print(f\"  {attr:<25}: {'âœ… å­˜åœ¨' if hasattr(data, attr) else 'âŒ ä¸å­˜åœ¨'}\")\n",
    "\n",
    "# # === æ©ç ç»Ÿè®¡ ===\n",
    "# if hasattr(data, \"train_mask\"):\n",
    "#     total_nodes = data.x.size(0)\n",
    "#     n_train = int(data.train_mask.sum())\n",
    "#     n_val   = int(data.val_mask.sum()) if hasattr(data, \"val_mask\") else 0\n",
    "#     n_test  = int(data.test_mask.sum()) if hasattr(data, \"test_mask\") else 0\n",
    "#     n_with  = int(data.train_withlabel_mask.sum()) if hasattr(data, \"train_withlabel_mask\") else 0\n",
    "#     n_nol   = int(data.train_nolabel_mask.sum()) if hasattr(data, \"train_nolabel_mask\") else 0\n",
    "\n",
    "#     print(\"\\nğŸ“Š æ©ç æ•°é‡éªŒè¯ï¼š\")\n",
    "#     print(f\"  train_mask         = {n_train}\")\n",
    "#     print(f\"  val_mask           = {n_val}\")\n",
    "#     print(f\"  test_mask          = {n_test}\")\n",
    "#     print(f\"  â””â”€ train_withlabel = {n_with}\")\n",
    "#     print(f\"  â””â”€ train_nolabel   = {n_nol}\")\n",
    "#     print(f\"  åˆè®¡èŠ‚ç‚¹æ•°         = {total_nodes}\")\n",
    "#     if n_train + n_val + n_test == total_nodes:\n",
    "#         print(\"âœ… æ©ç åˆ’åˆ†æ€»æ•°åŒ¹é…èŠ‚ç‚¹æ€»æ•°\")\n",
    "#     else:\n",
    "#         print(\"âš ï¸ æ³¨æ„ï¼šæ©ç åˆ’åˆ†èŠ‚ç‚¹æ•°ä¸æ€»æ•°ä¸å®Œå…¨åŒ¹é…ï¼Œå¯èƒ½å­˜åœ¨èˆå…¥è¯¯å·®ã€‚\")\n",
    "\n",
    "# # === æ ‡ç­¾ç»Ÿè®¡ ===\n",
    "# unique_labels = sorted(set(data.y.cpu().numpy().tolist()))\n",
    "# print(\"\\nğŸ¯ æ ‡ç­¾åˆ†å¸ƒæ£€æŸ¥ï¼š\")\n",
    "# print(f\"  æ ‡ç­¾ç±»åˆ«æ•°: {len(unique_labels)}\")\n",
    "# print(f\"  ç±»åˆ«ç¤ºä¾‹: {unique_labels[:10]}\")\n",
    "\n",
    "# print(\"\\nğŸ¯ æ©ç ä¸æ ‡ç­¾éªŒè¯å®Œæˆã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817eb662",
   "metadata": {},
   "source": [
    "#### time + cos_knn èåˆå»ºå›¾"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
