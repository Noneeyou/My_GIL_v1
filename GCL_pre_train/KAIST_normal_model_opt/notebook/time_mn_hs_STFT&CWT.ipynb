{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2f360b",
   "metadata": {},
   "source": [
    "# æ•°æ®è¯»å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61bd645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/src\n",
      "\n",
      "========== åŠ è½½ train å›¾ï¼ˆå¸¦ label maskï¼‰ ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118056/1724986247.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(train_graph_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ§© Graph Data Summary\n",
      "============================================================\n",
      "ğŸ“Š èŠ‚ç‚¹æ•°é‡ (num_nodes): 15000\n",
      "ğŸ“ˆ èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ (num_features): 4\n",
      "ğŸ”— è¾¹æ•°é‡ (num_edges): 6000000\n",
      "ğŸ” è‡ªç¯æ•°é‡ (self-loops): 0\n",
      "ğŸ¯ æ ‡ç­¾ç»´åº¦ (y_dim): torch.Size([15000])\n",
      "\n",
      "ğŸ§¾ Dataå¯¹è±¡åŒ…å«å­—æ®µ: ['train_nolabel_mask', 'train_withlabel_mask', 'x', 'y', 'edge_index']\n",
      "============================================================\n",
      "\n",
      "\n",
      "========== åŠ è½½ val å›¾ ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118056/1724986247.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_data = torch.load(val_graph_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== åŠ è½½ test å›¾ ==========\n",
      "ğŸ¯ å·²å®Œæˆ train_data.x æ ‡å‡†åŒ–\n",
      "ğŸ¯ å·²å®Œæˆ test_data.x æ ‡å‡†åŒ–\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118056/1724986247.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load(test_graph_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ å…±ç”Ÿæˆ 15 ä¸ªå­å›¾ batch\n"
     ]
    }
   ],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import subgraph, add_self_loops\n",
    "\n",
    "# === å¯¼å…¥æ¨¡å‹ç»“æ„ ===\n",
    "from models import GraphContrastiveLearner, augment_graph, summarize_graph\n",
    "\n",
    "\n",
    "# ===================== 0ï¸âƒ£ è·¯å¾„è®¾ç½® =====================\n",
    "train_graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/train_graph_with_labelmask.pt\"\n",
    "val_graph_path   = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg/graph.pt\"\n",
    "test_graph_path  = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg/graph.pt\"\n",
    "\n",
    "save_dir = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/model_save/time_morenode_high_similar_high_similar\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ===================== 1ï¸âƒ£ åŠ è½½å›¾æ•°æ® =====================\n",
    "print(\"\\n========== åŠ è½½ train å›¾ï¼ˆå¸¦ label maskï¼‰ ==========\")\n",
    "train_data = torch.load(train_graph_path)\n",
    "summarize_graph(train_data)\n",
    "\n",
    "print(\"\\n========== åŠ è½½ val å›¾ ==========\")\n",
    "val_data = torch.load(val_graph_path)\n",
    "\n",
    "print(\"\\n========== åŠ è½½ test å›¾ ==========\")\n",
    "test_data = torch.load(test_graph_path)\n",
    "\n",
    "\n",
    "# ===================== 2ï¸âƒ£ å¯¹ train_data.x åš Z-score æ ‡å‡†åŒ– =====================\n",
    "x_mean = train_data.x.mean(dim=0, keepdim=True)\n",
    "x_std  = train_data.x.std(dim=0, keepdim=True) + 1e-6\n",
    "train_data.x = (train_data.x - x_mean) / x_std\n",
    "\n",
    "print(\"ğŸ¯ å·²å®Œæˆ train_data.x æ ‡å‡†åŒ–\")\n",
    "\n",
    "x_mean = test_data.x.mean(dim=0, keepdim=True)\n",
    "x_std  = test_data.x.std(dim=0, keepdim=True) + 1e-6\n",
    "test_data.x = (test_data.x - x_mean) / x_std\n",
    "\n",
    "print(\"ğŸ¯ å·²å®Œæˆ test_data.x æ ‡å‡†åŒ–\")\n",
    "\n",
    "# ===================== 3ï¸âƒ£ æ„é€  batch å­å›¾ =====================\n",
    "batch_size = 1024\n",
    "num_nodes = train_data.num_nodes\n",
    "\n",
    "perm = torch.randperm(num_nodes)\n",
    "\n",
    "batches = []\n",
    "for i in range(0, num_nodes, batch_size):\n",
    "    node_idx = perm[i:i + batch_size]\n",
    "\n",
    "    # ---- æå–å­å›¾ï¼ˆå¿…é¡» CPUï¼‰----\n",
    "    sub_edge_index, _ = subgraph(\n",
    "        node_idx,\n",
    "        train_data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "\n",
    "    sub_x = train_data.x[node_idx]\n",
    "\n",
    "    # ---------- â­ åŠ å…¥è‡ªç¯ ----------\n",
    "    sub_edge_index, _ = add_self_loops(\n",
    "        sub_edge_index, num_nodes=sub_x.size(0)\n",
    "    )\n",
    "\n",
    "    sub_data = Data(\n",
    "        x=sub_x,\n",
    "        edge_index=sub_edge_index,\n",
    "    )\n",
    "    batches.append(sub_data)\n",
    "\n",
    "print(f\"ğŸ“Œ å…±ç”Ÿæˆ {len(batches)} ä¸ªå­å›¾ batch\")\n",
    "\n",
    "loader = DataLoader(batches, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6db6db",
   "metadata": {},
   "source": [
    "# STFT&CWT ç‰¹å¾å˜æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0192e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122805/3471273600.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(train_pt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "èŠ‚ç‚¹æ•°: 15000\n",
      "åŸå§‹ç‰¹å¾ç»´åº¦: (15000, 4)\n",
      "çª—å£æ•°é‡: 57\n",
      "æ¯çª—å£ STFT ç‰¹å¾ç»´åº¦: 132\n",
      "âœ… å·²ä¿å­˜ STFT+æ ‡ç­¾ ç‰¹å¾ CSVï¼š/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/high_similar/STFT&CWT/train_STFT_features.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ====================== 0. æ–‡ä»¶è·¯å¾„ ======================\n",
    "train_pt_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/train/pyg/train_graph_with_labelmask.pt\"\n",
    "save_csv_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/high_similar/STFT&CWT/train_STFT_features.csv\"\n",
    "\n",
    "# ====================== 1. åŠ è½½åŸå§‹ train.pt ======================\n",
    "data = torch.load(train_pt_path, map_location=\"cpu\")\n",
    "\n",
    "# åŸå§‹ä¿¡å·ï¼š (N,4)\n",
    "X = data.x.cpu().numpy()      \n",
    "N = len(X)\n",
    "\n",
    "# åŸå§‹æ ‡ç­¾ï¼šå‡è®¾ data.y æ˜¯ shape (N, )\n",
    "labels = data.y.cpu().numpy()\n",
    "\n",
    "print(\"èŠ‚ç‚¹æ•°:\", N)\n",
    "print(\"åŸå§‹ç‰¹å¾ç»´åº¦:\", X.shape)\n",
    "\n",
    "# ====================== 2. æ»‘çª—å‚æ•° ======================\n",
    "window = 512        \n",
    "step   = int(window * 0.5)    \n",
    "\n",
    "# ====================== 3. å¯¹æ¯ä¸ªçª—å£è¿›è¡Œ STFT + å†³å®šçª—å£ label ======================\n",
    "stft_features = []            \n",
    "window_indices = []          \n",
    "window_labels  = []           # æ–°å¢ï¼šçª—å£æœ€ç»ˆæ ‡ç­¾\n",
    "\n",
    "n_fft = 64\n",
    "hop = 32\n",
    "window_fn = torch.hann_window(n_fft)\n",
    "\n",
    "num_windows = 0\n",
    "\n",
    "for start in range(0, N - window + 1, step):\n",
    "\n",
    "    # ---- å–çª—å£å†…æ•°æ® ----\n",
    "    chunk = X[start:start + window]        # shape: (window,4)\n",
    "    chunk_label = labels[start:start + window]  # shape: (window,)\n",
    "\n",
    "    # ---- ç»Ÿè®¡çª—å£å†…æœ€é¢‘ç¹æ ‡ç­¾ ----\n",
    "    (unique, counts) = np.unique(chunk_label, return_counts=True)\n",
    "    win_label = unique[np.argmax(counts)]\n",
    "\n",
    "    window_feat = []                      \n",
    "\n",
    "    # ---- å¯¹æ¯é€šé“åš STFT ----\n",
    "    for ch in range(4):\n",
    "        sig = chunk[:, ch]                \n",
    "        xt = torch.tensor(sig)\n",
    "\n",
    "        stft_complex = torch.stft(\n",
    "            xt,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop,\n",
    "            win_length=n_fft,\n",
    "            window=window_fn,\n",
    "            return_complex=True\n",
    "        )\n",
    "\n",
    "        stft_mag = stft_complex.abs().numpy()    \n",
    "        stft_mean = stft_mag.mean(axis=1)        \n",
    "\n",
    "        window_feat.extend(stft_mean.tolist())\n",
    "\n",
    "    # ---- ä¿å­˜ç»“æœ ----\n",
    "    stft_features.append(window_feat)\n",
    "    window_indices.append(start)\n",
    "    window_labels.append(win_label)\n",
    "    num_windows += 1\n",
    "\n",
    "\n",
    "# ====================== 4. è½¬ä¸º DataFrame å¹¶ä¿å­˜ CSV ======================\n",
    "stft_features = np.array(stft_features)    \n",
    "freq_dim = stft_features.shape[1] // 4\n",
    "\n",
    "print(\"çª—å£æ•°é‡:\", num_windows)\n",
    "print(\"æ¯çª—å£ STFT ç‰¹å¾ç»´åº¦:\", stft_features.shape[1])\n",
    "\n",
    "# æ„å»ºåˆ—å\n",
    "columns = []\n",
    "for ch in range(4):\n",
    "    for f in range(freq_dim):\n",
    "        columns.append(f\"ch{ch+1}_freq{f}\")\n",
    "\n",
    "df = pd.DataFrame(stft_features, columns=columns)\n",
    "\n",
    "df.insert(0, \"window_start\", window_indices)   # è®°å½•çª—å£èµ·ç‚¹\n",
    "df[\"label\"] = window_labels                    # æ–°å¢ï¼šçª—å£æ ‡ç­¾\n",
    "\n",
    "df.to_csv(save_csv_path, index=False)\n",
    "\n",
    "print(f\"âœ… å·²ä¿å­˜ STFT+æ ‡ç­¾ ç‰¹å¾ CSVï¼š{save_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a3b78",
   "metadata": {},
   "source": [
    "# STFTå˜åŒ–åçš„train å»ºå›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96672e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± 57 ä¸ªèŠ‚ç‚¹ï¼Œ233 æ¡æ— å‘è¾¹\n",
      "ğŸ“ nodes.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/high_similar/STFT&CWT/STFT_pyg/train/nodes.csv\n",
      "ğŸ“ edges.csv: /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/high_similar/STFT&CWT/STFT_pyg/train/edges.csv\n",
      "ğŸ“ graph.pt : /home/charles/HZU/Data_processed/my_CIL_V1/KAIST/high_similar/STFT&CWT/STFT_pyg/train/graph.pt\n",
      "ğŸ§© ç‰¹å¾ç»´åº¦: 132 (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)\n"
     ]
    }
   ],
   "source": [
    "def build_local_temporal_graph(\n",
    "    csv_path: str,\n",
    "    save_dir: str,\n",
    "    num_edges: int = 10,\n",
    "    label_col: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    åŸºäºæ—¶é—´é¡ºåºæ„å»ºå±€éƒ¨æ—¶åºå›¾ã€‚\n",
    "    æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä¸Šä¸‹ç›¸é‚»æ ·æœ¬æ„æˆè¾¹ã€‚\n",
    "    æ ‡ç­¾åˆ—å¯æŒ‡å®šç´¢å¼•ï¼Œè‹¥ä¸æŒ‡å®šåˆ™é»˜è®¤æœ€åä¸€åˆ—ã€‚\n",
    "    ğŸš« è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ï¼ˆå¸¸ç”¨äºåºå·/IDï¼‰ï¼Œé¿å…è¯¯å…¥ç‰¹å¾è®¡ç®—ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "        csv_path (str): è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ã€‚\n",
    "        save_dir (str): å›¾ç»“æ„æ–‡ä»¶çš„ä¿å­˜æ–‡ä»¶å¤¹ã€‚\n",
    "        num_edges (int): æ¯ä¸ªèŠ‚ç‚¹çš„è¾¹æ•°ï¼ˆä¸Šä¸‹å¹³å‡åˆ†é…ï¼‰ã€‚\n",
    "        label_col (int): æ ‡ç­¾åˆ—ç´¢å¼•ï¼ˆé»˜è®¤ None â†’ æœ€åä¸€åˆ—ï¼‰ã€‚\n",
    "    è¿”å›:\n",
    "        (nodes_csv, edges_csv, graph_pt): ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å…ƒç»„ã€‚\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # === è¯»å–æ•°æ® ===\n",
    "    df = pd.read_csv(csv_path)\n",
    "    num_nodes = len(df)\n",
    "    if num_nodes == 0:\n",
    "        raise ValueError(\"âŒ è¾“å…¥ CSV æ–‡ä»¶ä¸ºç©ºã€‚\")\n",
    "\n",
    "    # === æ ‡ç­¾åˆ—åˆ¤æ–­ ===\n",
    "    if label_col is None:\n",
    "        label_col = df.shape[1] - 1\n",
    "\n",
    "    # === æå–æ ‡ç­¾åˆ— ===\n",
    "    y = torch.tensor(df.iloc[:, label_col].values, dtype=torch.long)\n",
    "\n",
    "    # === æ„é€ ç‰¹å¾åˆ—ï¼ˆå»æ‰é¦–åˆ— + æ ‡ç­¾åˆ—ï¼‰===\n",
    "    drop_cols = [df.columns[0], df.columns[label_col]] if label_col != 0 else [df.columns[0]]\n",
    "    df_features = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "    # ä¿ç•™æ•°å€¼åˆ—\n",
    "    df_features = df_features.select_dtypes(include=[\"float\", \"int\"])\n",
    "    if df_features.shape[1] == 0:\n",
    "        raise ValueError(\"âŒ ç‰¹å¾åˆ—ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥ CSVã€‚\")\n",
    "\n",
    "    # === æ„å»ºè¾¹ ===\n",
    "    half = num_edges // 2\n",
    "    edges = []\n",
    "    for i in range(num_nodes):\n",
    "        start_up = max(0, i - half)\n",
    "        end_down = min(num_nodes, i + half + 1)\n",
    "        up_neighbors = list(range(start_up, i))\n",
    "        down_neighbors = list(range(i + 1, end_down))\n",
    "\n",
    "        total_needed = num_edges\n",
    "        current = len(up_neighbors) + len(down_neighbors)\n",
    "        if current < total_needed:\n",
    "            remaining = total_needed - current\n",
    "            if i + half + 1 >= num_nodes:  # ä¸‹æ–¹ä¸å¤Ÿ\n",
    "                extra_up = list(range(max(0, start_up - remaining), start_up))\n",
    "                up_neighbors = extra_up + up_neighbors\n",
    "            elif i - half < 0:  # ä¸Šæ–¹ä¸å¤Ÿ\n",
    "                extra_down = list(range(end_down, min(num_nodes, end_down + remaining)))\n",
    "                down_neighbors += extra_down\n",
    "\n",
    "        for j in up_neighbors + down_neighbors:\n",
    "            edges.append((i, j))\n",
    "            edges.append((j, i))\n",
    "\n",
    "    # === ä¿å­˜èŠ‚ç‚¹ä¸è¾¹ ===\n",
    "    nodes_path = os.path.join(save_dir, \"nodes.csv\")\n",
    "    edges_path = os.path.join(save_dir, \"edges.csv\")\n",
    "    graph_path = os.path.join(save_dir, \"graph.pt\")\n",
    "\n",
    "    df_features.to_csv(nodes_path, index=False)\n",
    "    pd.DataFrame(edges, columns=[\"source\", \"target\"]).to_csv(edges_path, index=False)\n",
    "\n",
    "    # === æ„å»º PyG å›¾ç»“æ„ ===\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).T\n",
    "    x = torch.tensor(df_features.values, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "    torch.save(data, graph_path)\n",
    "\n",
    "    print(f\"âœ… å›¾ç»“æ„æ„å»ºå®Œæˆï¼Œå…± {num_nodes} ä¸ªèŠ‚ç‚¹ï¼Œ{len(edges)//2} æ¡æ— å‘è¾¹\")\n",
    "    print(f\"ğŸ“ nodes.csv: {nodes_path}\")\n",
    "    print(f\"ğŸ“ edges.csv: {edges_path}\")\n",
    "    print(f\"ğŸ“ graph.pt : {graph_path}\")\n",
    "    print(f\"ğŸ§© ç‰¹å¾ç»´åº¦: {x.shape[1]} (å·²è‡ªåŠ¨å¿½ç•¥é¦–åˆ—ä¸æ ‡ç­¾åˆ—)\")\n",
    "\n",
    "    return nodes_path, edges_path, graph_path\n",
    "\n",
    "\n",
    "\n",
    "# === è°ƒç”¨æœ¬åœ°æ—¶åºå»ºå›¾å‡½æ•° ===\n",
    "csv_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/high_similar/STFT&CWT/train_STFT_features.csv\"\n",
    "pyg_dir = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/high_similar/STFT&CWT/STFT_pyg/train\"\n",
    "num_edges =  5\n",
    "nodes_csv, edges_csv, graph_pt = build_local_temporal_graph(\n",
    "    csv_path=csv_path,\n",
    "    save_dir=pyg_dir,\n",
    "    num_edges=num_edges\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec0ec9",
   "metadata": {},
   "source": [
    "# è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2b116",
   "metadata": {},
   "source": [
    "## æ¨¡å‹åˆå§‹åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941419bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/src\n",
      "\n",
      "========== åŠ è½½ train å›¾ï¼ˆå¸¦ label maskï¼‰ ==========\n",
      "\n",
      "============================================================\n",
      "ğŸ§© Graph Data Summary\n",
      "============================================================\n",
      "ğŸ“Š èŠ‚ç‚¹æ•°é‡ (num_nodes): 57\n",
      "ğŸ“ˆ èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ (num_features): 132\n",
      "ğŸ”— è¾¹æ•°é‡ (num_edges): 466\n",
      "ğŸ” è‡ªç¯æ•°é‡ (self-loops): 0\n",
      "ğŸ¯ æ ‡ç­¾ç»´åº¦ (y_dim): torch.Size([57])\n",
      "\n",
      "ğŸ§¾ Dataå¯¹è±¡åŒ…å«å­—æ®µ: ['edge_index', 'y', 'x']\n",
      "============================================================\n",
      "\n",
      "ğŸ¯ å·²å®Œæˆ train_data.x æ ‡å‡†åŒ–\n",
      "ğŸ“Œ å…±ç”Ÿæˆ 1 ä¸ªå­å›¾ batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122805/1867446096.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(train_graph_path)\n"
     ]
    }
   ],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import subgraph, add_self_loops\n",
    "\n",
    "# === å¯¼å…¥æ¨¡å‹ç»“æ„ ===\n",
    "from models import GraphContrastiveLearner, augment_graph, summarize_graph\n",
    "\n",
    "\n",
    "# ===================== 0ï¸âƒ£ è·¯å¾„è®¾ç½® =====================\n",
    "train_graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/high_similar/STFT&CWT/STFT_pyg/train/graph.pt\"\n",
    "# val_graph_path   = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/val/pyg/graph.pt\"\n",
    "# test_graph_path  = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/all_class_morenode_high_similar/test/pyg/graph.pt\"\n",
    "\n",
    "save_dir = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/model_save/time_mn_hs_STFT&CWT\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ===================== 1ï¸âƒ£ åŠ è½½å›¾æ•°æ® =====================\n",
    "print(\"\\n========== åŠ è½½ train å›¾ï¼ˆå¸¦ label maskï¼‰ ==========\")\n",
    "train_data = torch.load(train_graph_path)\n",
    "summarize_graph(train_data)\n",
    "\n",
    "# print(\"\\n========== åŠ è½½ val å›¾ ==========\")\n",
    "# val_data = torch.load(val_graph_path)\n",
    "\n",
    "# print(\"\\n========== åŠ è½½ test å›¾ ==========\")\n",
    "# test_data = torch.load(test_graph_path)\n",
    "\n",
    "\n",
    "# ===================== 2ï¸âƒ£ å¯¹ train_data.x åš Z-score æ ‡å‡†åŒ– =====================\n",
    "x_mean = train_data.x.mean(dim=0, keepdim=True)\n",
    "x_std  = train_data.x.std(dim=0, keepdim=True) + 1e-6\n",
    "train_data.x = (train_data.x - x_mean) / x_std\n",
    "\n",
    "print(\"ğŸ¯ å·²å®Œæˆ train_data.x æ ‡å‡†åŒ–\")\n",
    "\n",
    "# x_mean = test_data.x.mean(dim=0, keepdim=True)\n",
    "# x_std  = test_data.x.std(dim=0, keepdim=True) + 1e-6\n",
    "# test_data.x = (test_data.x - x_mean) / x_std\n",
    "\n",
    "# print(\"ğŸ¯ å·²å®Œæˆ test_data.x æ ‡å‡†åŒ–\")\n",
    "\n",
    "# ===================== 3ï¸âƒ£ æ„é€  batch å­å›¾ =====================\n",
    "batch_size = 57\n",
    "num_nodes = train_data.num_nodes\n",
    "\n",
    "perm = torch.randperm(num_nodes)\n",
    "\n",
    "batches = []\n",
    "for i in range(0, num_nodes, batch_size):\n",
    "    node_idx = perm[i:i + batch_size]\n",
    "\n",
    "    # ---- æå–å­å›¾ï¼ˆå¿…é¡» CPUï¼‰----\n",
    "    sub_edge_index, _ = subgraph(\n",
    "        node_idx,\n",
    "        train_data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "\n",
    "    sub_x = train_data.x[node_idx]\n",
    "\n",
    "    # ---------- â­ åŠ å…¥è‡ªç¯ ----------\n",
    "    sub_edge_index, _ = add_self_loops(\n",
    "        sub_edge_index, num_nodes=sub_x.size(0)\n",
    "    )\n",
    "\n",
    "    sub_data = Data(\n",
    "        x=sub_x,\n",
    "        edge_index=sub_edge_index,\n",
    "    )\n",
    "    batches.append(sub_data)\n",
    "\n",
    "print(f\"ğŸ“Œ å…±ç”Ÿæˆ {len(batches)} ä¸ªå­å›¾ batch\")\n",
    "\n",
    "loader = DataLoader(batches, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "# ===================== 4ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹ =====================\n",
    "in_dim = train_data.x.size(1)\n",
    "hidden_dim = 256\n",
    "out_dim = 256\n",
    "proj_dim = 128\n",
    "tau = 0.5\n",
    "\n",
    "model = GraphContrastiveLearner(in_dim, hidden_dim, out_dim, proj_dim, tau).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f0f79",
   "metadata": {},
   "source": [
    "## GCLè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e345001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= ğŸš€ å¼€å§‹ Batch å¯¹æ¯”å­¦ä¹ è®­ç»ƒ =================\n",
      "\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 1 | Loss=4.0717\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 2 | Loss=3.6029\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 3 | Loss=3.3721\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 4 | Loss=3.2488\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 5 | Loss=3.1354\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 6 | Loss=3.1142\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 7 | Loss=3.0675\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 8 | Loss=3.0432\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 9 | Loss=3.0015\n",
      "Epoch [010/200] | InfoNCE Loss: 2.9937\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 10 | Loss=2.9937\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 11 | Loss=2.9805\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 12 | Loss=2.9674\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 13 | Loss=2.9498\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 14 | Loss=2.9367\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 15 | Loss=2.9292\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 16 | Loss=2.9103\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 17 | Loss=2.9080\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 18 | Loss=2.8947\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 19 | Loss=2.8797\n",
      "Epoch [020/200] | InfoNCE Loss: 2.8723\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 20 | Loss=2.8723\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 21 | Loss=2.8703\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 22 | Loss=2.8554\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 23 | Loss=2.8481\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 24 | Loss=2.8435\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 25 | Loss=2.8371\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 26 | Loss=2.8225\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 27 | Loss=2.8149\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 28 | Loss=2.8070\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 29 | Loss=2.7997\n",
      "Epoch [030/200] | InfoNCE Loss: 2.8004\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 31 | Loss=2.7880\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 32 | Loss=2.7778\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 33 | Loss=2.7685\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 34 | Loss=2.7677\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 35 | Loss=2.7558\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 36 | Loss=2.7465\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 37 | Loss=2.7434\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 38 | Loss=2.7419\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 39 | Loss=2.7306\n",
      "Epoch [040/200] | InfoNCE Loss: 2.7253\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 40 | Loss=2.7253\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 41 | Loss=2.7209\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 42 | Loss=2.7045\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 43 | Loss=2.7039\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 44 | Loss=2.6983\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 45 | Loss=2.6932\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 46 | Loss=2.6850\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 47 | Loss=2.6747\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 48 | Loss=2.6686\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 49 | Loss=2.6668\n",
      "Epoch [050/200] | InfoNCE Loss: 2.6564\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 50 | Loss=2.6564\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 51 | Loss=2.6525\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 52 | Loss=2.6434\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 53 | Loss=2.6405\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 54 | Loss=2.6332\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 55 | Loss=2.6285\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 56 | Loss=2.6202\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 57 | Loss=2.6181\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 58 | Loss=2.6107\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 59 | Loss=2.6076\n",
      "Epoch [060/200] | InfoNCE Loss: 2.5960\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 60 | Loss=2.5960\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 61 | Loss=2.5884\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 62 | Loss=2.5838\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 63 | Loss=2.5810\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 64 | Loss=2.5696\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 65 | Loss=2.5691\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 66 | Loss=2.5613\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 67 | Loss=2.5520\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 68 | Loss=2.5487\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 69 | Loss=2.5407\n",
      "Epoch [070/200] | InfoNCE Loss: 2.5351\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 70 | Loss=2.5351\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 71 | Loss=2.5287\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 72 | Loss=2.5255\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 73 | Loss=2.5189\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 74 | Loss=2.5149\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 75 | Loss=2.4987\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 76 | Loss=2.4963\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 77 | Loss=2.4904\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 78 | Loss=2.4796\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 79 | Loss=2.4777\n",
      "Epoch [080/200] | InfoNCE Loss: 2.4730\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 80 | Loss=2.4730\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 81 | Loss=2.4667\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 82 | Loss=2.4629\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 83 | Loss=2.4508\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 84 | Loss=2.4492\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 85 | Loss=2.4408\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 86 | Loss=2.4284\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 87 | Loss=2.4253\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 88 | Loss=2.4201\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 89 | Loss=2.4137\n",
      "Epoch [090/200] | InfoNCE Loss: 2.4053\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 90 | Loss=2.4053\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 91 | Loss=2.3968\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 92 | Loss=2.3906\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 93 | Loss=2.3881\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 94 | Loss=2.3774\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 95 | Loss=2.3712\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 96 | Loss=2.3639\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 97 | Loss=2.3578\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 98 | Loss=2.3524\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 99 | Loss=2.3438\n",
      "Epoch [100/200] | InfoNCE Loss: 2.3363\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 100 | Loss=2.3363\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 101 | Loss=2.3315\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 102 | Loss=2.3253\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 103 | Loss=2.3175\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 104 | Loss=2.3064\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 105 | Loss=2.2998\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 106 | Loss=2.2929\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 107 | Loss=2.2856\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 108 | Loss=2.2809\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 109 | Loss=2.2713\n",
      "Epoch [110/200] | InfoNCE Loss: 2.2650\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 110 | Loss=2.2650\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 111 | Loss=2.2559\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 112 | Loss=2.2498\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 113 | Loss=2.2393\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 114 | Loss=2.2377\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 115 | Loss=2.2269\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 116 | Loss=2.2150\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 117 | Loss=2.2105\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 118 | Loss=2.2020\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 119 | Loss=2.2013\n",
      "Epoch [120/200] | InfoNCE Loss: 2.1864\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 120 | Loss=2.1864\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 121 | Loss=2.1838\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 122 | Loss=2.1726\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 123 | Loss=2.1655\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 124 | Loss=2.1553\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 125 | Loss=2.1471\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 126 | Loss=2.1385\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 127 | Loss=2.1273\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 128 | Loss=2.1250\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 129 | Loss=2.1129\n",
      "Epoch [130/200] | InfoNCE Loss: 2.1071\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 130 | Loss=2.1071\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 131 | Loss=2.0992\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 132 | Loss=2.0891\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 133 | Loss=2.0774\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 134 | Loss=2.0716\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 135 | Loss=2.0621\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 136 | Loss=2.0575\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 137 | Loss=2.0458\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 138 | Loss=2.0367\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 139 | Loss=2.0310\n",
      "Epoch [140/200] | InfoNCE Loss: 2.0203\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 140 | Loss=2.0203\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 141 | Loss=2.0117\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 142 | Loss=2.0039\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 143 | Loss=1.9945\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 144 | Loss=1.9857\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 145 | Loss=1.9743\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 146 | Loss=1.9606\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 147 | Loss=1.9532\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 148 | Loss=1.9436\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 149 | Loss=1.9337\n",
      "Epoch [150/200] | InfoNCE Loss: 1.9262\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 150 | Loss=1.9262\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 151 | Loss=1.9224\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 152 | Loss=1.9074\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 153 | Loss=1.9002\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 154 | Loss=1.8890\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 155 | Loss=1.8786\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 156 | Loss=1.8684\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 157 | Loss=1.8557\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 158 | Loss=1.8492\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 159 | Loss=1.8409\n",
      "Epoch [160/200] | InfoNCE Loss: 1.8282\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 160 | Loss=1.8282\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 161 | Loss=1.8200\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 162 | Loss=1.8076\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 163 | Loss=1.7964\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 164 | Loss=1.7862\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 165 | Loss=1.7777\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 166 | Loss=1.7695\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 167 | Loss=1.7585\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 168 | Loss=1.7497\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 169 | Loss=1.7333\n",
      "Epoch [170/200] | InfoNCE Loss: 1.7245\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 170 | Loss=1.7245\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 171 | Loss=1.7180\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 172 | Loss=1.7050\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 173 | Loss=1.6943\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 174 | Loss=1.6830\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 175 | Loss=1.6733\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 176 | Loss=1.6615\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 177 | Loss=1.6499\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 178 | Loss=1.6385\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 179 | Loss=1.6306\n",
      "Epoch [180/200] | InfoNCE Loss: 1.6202\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 180 | Loss=1.6202\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 181 | Loss=1.6084\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 182 | Loss=1.5932\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 183 | Loss=1.5861\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 184 | Loss=1.5747\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 185 | Loss=1.5640\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 186 | Loss=1.5529\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 187 | Loss=1.5403\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 188 | Loss=1.5277\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 189 | Loss=1.5173\n",
      "Epoch [190/200] | InfoNCE Loss: 1.5038\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 190 | Loss=1.5038\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 191 | Loss=1.4920\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 192 | Loss=1.4798\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 193 | Loss=1.4711\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 194 | Loss=1.4621\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 195 | Loss=1.4514\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 196 | Loss=1.4352\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 197 | Loss=1.4258\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 198 | Loss=1.4147\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 199 | Loss=1.4022\n",
      "Epoch [200/200] | InfoNCE Loss: 1.3919\n",
      "ğŸ’¾ [BEST MODEL UPDATED] Epoch 200 | Loss=1.3919\n",
      "\n",
      "ğŸ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: /home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/model_save/time_mn_hs_STFT&CWT/KAIST_normal_pretrain_epoch200.pt\n",
      "ğŸ† æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜è‡³: /home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/model_save/time_mn_hs_STFT&CWT/KAIST_normal_pretrain_best.pt | best_loss=1.3919\n"
     ]
    }
   ],
   "source": [
    "# ===================== 5ï¸âƒ£ Batch è®­ç»ƒ =====================\n",
    "epochs = 200\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model_path = os.path.join(save_dir, f\"KAIST_normal_pretrain_best.pt\")\n",
    "\n",
    "print(\"\\n================= ğŸš€ å¼€å§‹ Batch å¯¹æ¯”å­¦ä¹ è®­ç»ƒ =================\\n\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data[0].to(device)   # batch_size=1 â†’ å–ç¬¬ 0 ä¸ª\n",
    "\n",
    "        # ---- ç”Ÿæˆä¸¤ä»½å¢å¼ºè§†å›¾ ----\n",
    "        data1 = augment_graph(\n",
    "            batch_data,\n",
    "            feature_drop_prob=0.2,\n",
    "            edge_drop_prob=0.1,\n",
    "            noise_std=0.02\n",
    "        ).to(device)\n",
    "\n",
    "        data2 = augment_graph(\n",
    "            batch_data,\n",
    "            feature_drop_prob=0.2,\n",
    "            edge_drop_prob=0.1,\n",
    "            noise_std=0.02\n",
    "        ).to(device)\n",
    "\n",
    "        # ---- è®¡ç®— InfoNCE å¯¹æ¯”æŸå¤± ----\n",
    "        loss = model.compute_loss(\n",
    "            data1.x, data1.edge_index,\n",
    "            data2.x, data2.edge_index\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # ======== æ—¥å¿—è¾“å‡º ========\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch:03d}/{epochs}] | InfoNCE Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # ======== ä¿å­˜å½“å‰æœ€ä¼˜æ¨¡å‹ ========\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"best_loss\": best_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"config\": {\n",
    "                \"in_dim\": in_dim,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"out_dim\": out_dim,\n",
    "                \"proj_dim\": proj_dim,\n",
    "                \"tau\": tau\n",
    "            }\n",
    "        }, best_model_path)\n",
    "\n",
    "        print(f\"ğŸ’¾ [BEST MODEL UPDATED] Epoch {epoch} | Loss={total_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ===================== æœ€ç»ˆæ¨¡å‹ä¿å­˜ =====================\n",
    "final_path = os.path.join(save_dir, f\"KAIST_normal_pretrain_epoch{epochs}.pt\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"epoch\": epochs,\n",
    "    \"config\": {\n",
    "        \"in_dim\": in_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"out_dim\": out_dim,\n",
    "        \"proj_dim\": proj_dim,\n",
    "        \"tau\": tau\n",
    "    }\n",
    "}, final_path)\n",
    "\n",
    "print(f\"\\nğŸ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}\")\n",
    "print(f\"ğŸ† æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path} | best_loss={best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18c30d",
   "metadata": {},
   "source": [
    "## åˆ†ç±»å¤´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e903b72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/src\n",
      "ğŸš€ æå–è®­ç»ƒæ•°æ®ç‰¹å¾ï¼ˆEncoder forwardï¼‰...\n",
      "ğŸ“Œ ä½¿ç”¨å…¨éƒ¨æ ·æœ¬è¿›è¡Œè®­ç»ƒ: 57\n",
      "ğŸ“Œ è¡¨å¾ç»´åº¦(out_dim): 256\n",
      "\n",
      "================== ğŸ”¥ å¼€å§‹ä¸‹æ¸¸åˆ†ç±»è®­ç»ƒï¼ˆæ•´å›¾ + å…¨éƒ¨æ•°æ®ï¼‰ ==================\n",
      "\n",
      "Epoch 005/200 | Loss: 2.1915\n",
      "Epoch 010/200 | Loss: 1.5507\n",
      "Epoch 015/200 | Loss: 0.9435\n",
      "Epoch 020/200 | Loss: 0.4819\n",
      "Epoch 025/200 | Loss: 0.2175\n",
      "Epoch 030/200 | Loss: 0.0970\n",
      "Epoch 035/200 | Loss: 0.0481\n",
      "Epoch 040/200 | Loss: 0.0277\n",
      "Epoch 045/200 | Loss: 0.0181\n",
      "Epoch 050/200 | Loss: 0.0131\n",
      "Epoch 055/200 | Loss: 0.0101\n",
      "Epoch 060/200 | Loss: 0.0083\n",
      "Epoch 065/200 | Loss: 0.0071\n",
      "Epoch 070/200 | Loss: 0.0063\n",
      "Epoch 075/200 | Loss: 0.0056\n",
      "Epoch 080/200 | Loss: 0.0052\n",
      "Epoch 085/200 | Loss: 0.0048\n",
      "Epoch 090/200 | Loss: 0.0044\n",
      "Epoch 095/200 | Loss: 0.0041\n",
      "Epoch 100/200 | Loss: 0.0039\n",
      "Epoch 105/200 | Loss: 0.0037\n",
      "Epoch 110/200 | Loss: 0.0035\n",
      "Epoch 115/200 | Loss: 0.0033\n",
      "Epoch 120/200 | Loss: 0.0031\n",
      "Epoch 125/200 | Loss: 0.0030\n",
      "Epoch 130/200 | Loss: 0.0028\n",
      "Epoch 135/200 | Loss: 0.0027\n",
      "Epoch 140/200 | Loss: 0.0026\n",
      "Epoch 145/200 | Loss: 0.0024\n",
      "Epoch 150/200 | Loss: 0.0023\n",
      "Epoch 155/200 | Loss: 0.0022\n",
      "Epoch 160/200 | Loss: 0.0021\n",
      "Epoch 165/200 | Loss: 0.0021\n",
      "Epoch 170/200 | Loss: 0.0020\n",
      "Epoch 175/200 | Loss: 0.0019\n",
      "Epoch 180/200 | Loss: 0.0018\n",
      "Epoch 185/200 | Loss: 0.0018\n",
      "Epoch 190/200 | Loss: 0.0017\n",
      "Epoch 195/200 | Loss: 0.0016\n",
      "Epoch 200/200 | Loss: 0.0016\n",
      "\n",
      "ğŸ ä¸‹æ¸¸çº¿æ€§åˆ†ç±»è®­ç»ƒå®Œæˆï¼\n",
      "ğŸ’¾ ä¸‹æ¸¸åˆ†ç±»å™¨å·²ä¿å­˜ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/model_save/time_mn_hs_STFT&CWT/downstream/downstream_classifier.pt\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "#        â­ å• Cellï¼šä½¿ç”¨ å…¨éƒ¨è®­ç»ƒæ•°æ® æ•´å›¾è®­ç»ƒä¸‹æ¸¸åˆ†ç±»å™¨ â­\n",
    "# ======================================================================\n",
    "%run ../_init_path.py\n",
    "from models import GraphContrastiveLearner, DownstreamClassifier\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ======================================================================\n",
    "# 1. ä» encoder æå– h_train\n",
    "# ======================================================================\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"ğŸš€ æå–è®­ç»ƒæ•°æ®ç‰¹å¾ï¼ˆEncoder forwardï¼‰...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    h_train, _ = model(train_data.x, train_data.edge_index)\n",
    "\n",
    "h_train = h_train.detach()         # [N, out_dim]\n",
    "y_all = train_data.y               # [N]\n",
    "\n",
    "# ========== â­ ä¸ä½¿ç”¨ maskï¼Œä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ ·æœ¬ ==========\n",
    "h_labeled = h_train\n",
    "y_labeled = y_all\n",
    "\n",
    "print(f\"ğŸ“Œ ä½¿ç”¨å…¨éƒ¨æ ·æœ¬è¿›è¡Œè®­ç»ƒ: {h_labeled.shape[0]}\")\n",
    "print(f\"ğŸ“Œ è¡¨å¾ç»´åº¦(out_dim): {h_labeled.shape[1]}\")\n",
    "\n",
    "# ======================================================================\n",
    "# 2. å®šä¹‰åˆ†ç±»å¤´ï¼ˆå…¨å›¾è®­ç»ƒï¼‰\n",
    "# ======================================================================\n",
    "\n",
    "num_features = h_labeled.size(1)\n",
    "num_classes = int(y_labeled.max().item() + 1)\n",
    "\n",
    "classifier = DownstreamClassifier(\n",
    "    in_dim=num_features,\n",
    "    num_classes=num_classes,\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# ======================================================================\n",
    "# 3. æ•´å›¾è®­ç»ƒ\n",
    "# ======================================================================\n",
    "\n",
    "epochs = 200\n",
    "print(\"\\n================== ğŸ”¥ å¼€å§‹ä¸‹æ¸¸åˆ†ç±»è®­ç»ƒï¼ˆæ•´å›¾ + å…¨éƒ¨æ•°æ®ï¼‰ ==================\\n\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = classifier(h_labeled)\n",
    "    loss = F.cross_entropy(logits, y_labeled)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:03d}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nğŸ ä¸‹æ¸¸çº¿æ€§åˆ†ç±»è®­ç»ƒå®Œæˆï¼\")\n",
    "\n",
    "# ======================================================================\n",
    "# 4. ä¿å­˜åˆ†ç±»å™¨\n",
    "# ======================================================================\n",
    "\n",
    "save_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/model_save/time_mn_hs_STFT&CWT/downstream/downstream_classifier.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"state_dict\": classifier.state_dict(),\n",
    "    \"in_dim\": num_features,\n",
    "    \"num_classes\": num_classes\n",
    "}, save_path)\n",
    "\n",
    "print(f\"ğŸ’¾ ä¸‹æ¸¸åˆ†ç±»å™¨å·²ä¿å­˜ï¼š{save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f96a0",
   "metadata": {},
   "source": [
    "# accæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6afe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ æ¨¡å‹è·¯å¾„å·²è®¾ç½®\n",
      "âœ… ä¸Šæ¸¸æ¨¡å‹åŠ è½½æˆåŠŸï¼\n",
      "âœ… ä¸‹æ¸¸åˆ†ç±»å™¨åŠ è½½æˆåŠŸï¼\n",
      "ğŸ“Œ Train å›¾æ•°æ®å·²åŠ è½½è‡³ GPU/CPU\n",
      "ğŸ”§ å·²æå– h_train\n",
      "\n",
      "ğŸ¯ Train Accuracy (ALL train nodes) = 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122805/2087874937.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(pretrain_path, map_location=device)\n",
      "/tmp/ipykernel_122805/2087874937.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  clf_ckpt = torch.load(classifier_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================== TRAIN METRICS ======================\n",
      "Accuracy : 100.00%\n",
      "Precision: 100.00%\n",
      "Recall   : 100.00%\n",
      "F1-score : 100.00%\n",
      "===========================================================\n",
      "\n",
      "============== TRAIN WRONG PREDICTIONS ==============\n",
      "âŒ é”™è¯¯æ ·æœ¬æ•°é‡ï¼š0 / 57\n",
      "\n",
      "ğŸ” Train æ¯ç±»é”™è¯¯ç»Ÿè®¡ï¼š(çœŸæ ‡ç­¾ -> é¢„æµ‹æ ‡ç­¾)\n",
      "\n",
      "ğŸ Train-only è¯„ä¼°å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "#    â­ å• Cellï¼šåŠ è½½ä¸Šæ¸¸æ¨¡å‹ + ä¸‹æ¸¸åˆ†ç±»å™¨ï¼Œå¹¶ç”¨ Train åšæµ‹è¯• â­\n",
    "# ======================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ====================== æ¨¡å‹è·¯å¾„ ======================\n",
    "pretrain_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/model_save/time_mn_hs_STFT&CWT/KAIST_normal_pretrain_best.pt\"\n",
    "\n",
    "classifier_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model_opt/model_save/time_mn_hs_STFT&CWT/downstream/downstream_classifier.pt\"\n",
    "\n",
    "print(\"ğŸ“Œ æ¨¡å‹è·¯å¾„å·²è®¾ç½®\")\n",
    "\n",
    "\n",
    "# ====================== åŠ è½½æ¨¡å‹ç»“æ„ ======================\n",
    "from models import GraphContrastiveLearner, DownstreamClassifier\n",
    "\n",
    "# ------- åŠ è½½ä¸Šæ¸¸ GCL æ¨¡å‹ -------\n",
    "ckpt = torch.load(pretrain_path, map_location=device)\n",
    "cfg = ckpt[\"config\"]\n",
    "\n",
    "model = GraphContrastiveLearner(\n",
    "    in_dim     = cfg[\"in_dim\"],\n",
    "    hidden_dim = cfg[\"hidden_dim\"],\n",
    "    out_dim    = cfg[\"out_dim\"],\n",
    "    proj_dim   = cfg[\"proj_dim\"],\n",
    "    tau        = cfg[\"tau\"] if \"tau\" in cfg else 0.5\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()\n",
    "print(\"âœ… ä¸Šæ¸¸æ¨¡å‹åŠ è½½æˆåŠŸï¼\")\n",
    "\n",
    "\n",
    "# ------- åŠ è½½ä¸‹æ¸¸åˆ†ç±»å™¨ -------\n",
    "clf_ckpt = torch.load(classifier_path, map_location=device)\n",
    "in_dim = clf_ckpt[\"in_dim\"]\n",
    "num_classes = clf_ckpt[\"num_classes\"]\n",
    "\n",
    "classifier = DownstreamClassifier(\n",
    "    in_dim=in_dim,\n",
    "    num_classes=num_classes,\n",
    "    hidden_dim=128\n",
    ").to(device)\n",
    "\n",
    "classifier.load_state_dict(clf_ckpt[\"state_dict\"])\n",
    "classifier.eval()\n",
    "print(\"âœ… ä¸‹æ¸¸åˆ†ç±»å™¨åŠ è½½æˆåŠŸï¼\")\n",
    "\n",
    "\n",
    "# ====================== åŠ è½½ Train å›¾ï¼ˆä½ å…ˆæ‰‹åŠ¨åŠ è½½ train_dataï¼‰ ======================\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "print(\"ğŸ“Œ Train å›¾æ•°æ®å·²åŠ è½½è‡³ GPU/CPU\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#          â­ 1ï¸âƒ£ æå– Train çš„ GCL encoder è¡¨å¾ h\n",
    "# ======================================================================\n",
    "with torch.no_grad():\n",
    "    h_train, _ = model(train_data.x, train_data.edge_index)\n",
    "\n",
    "print(\"ğŸ”§ å·²æå– h_train\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#          â­ 2ï¸âƒ£ Train Accuracyï¼ˆå…¨éƒ¨è®­ç»ƒèŠ‚ç‚¹ï¼‰\n",
    "# ======================================================================\n",
    "y_train = train_data.y\n",
    "with torch.no_grad():\n",
    "    logits_train = classifier(h_train)\n",
    "    pred_train = logits_train.argmax(dim=1)\n",
    "\n",
    "acc_train = (pred_train == y_train).float().mean().item()\n",
    "\n",
    "print(f\"\\nğŸ¯ Train Accuracy (ALL train nodes) = {acc_train*100:.2f}%\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#          â­ 3ï¸âƒ£ Train metricsï¼ˆprecision/recall/f1ï¼‰\n",
    "# ======================================================================\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "y_true_train = y_train.cpu().numpy()\n",
    "y_pred_train = pred_train.cpu().numpy()\n",
    "\n",
    "train_acc  = accuracy_score(y_true_train, y_pred_train)\n",
    "train_prec = precision_score(y_true_train, y_pred_train, average=\"macro\", zero_division=0)\n",
    "train_reca = recall_score(y_true_train, y_pred_train, average=\"macro\", zero_division=0)\n",
    "train_f1   = f1_score(y_true_train, y_pred_train, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"\\n====================== TRAIN METRICS ======================\")\n",
    "print(f\"Accuracy : {train_acc*100:.2f}%\")\n",
    "print(f\"Precision: {train_prec*100:.2f}%\")\n",
    "print(f\"Recall   : {train_reca*100:.2f}%\")\n",
    "print(f\"F1-score : {train_f1*100:.2f}%\")\n",
    "print(\"===========================================================\\n\")\n",
    "\n",
    "\n",
    "# ======================================================================\n",
    "#          â­ 4ï¸âƒ£ ç»Ÿè®¡è®­ç»ƒé›†çš„é”™è¯¯æ ·æœ¬\n",
    "# ======================================================================\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "wrong_train_idx = np.where(y_pred_train != y_true_train)[0]\n",
    "\n",
    "print(\"============== TRAIN WRONG PREDICTIONS ==============\")\n",
    "print(f\"âŒ é”™è¯¯æ ·æœ¬æ•°é‡ï¼š{len(wrong_train_idx)} / {len(y_true_train)}\")\n",
    "\n",
    "max_show = 20\n",
    "for i in wrong_train_idx[:max_show]:\n",
    "    print(f\"Index {i:4d} | True = {y_true_train[i]} | Pred = {y_pred_train[i]}\")\n",
    "if len(wrong_train_idx) > max_show:\n",
    "    print(f\"... (å…± {len(wrong_train_idx)} ä¸ªé”™æ ·æœ¬ï¼Œåªæ˜¾ç¤ºå‰ {max_show} ä¸ª)\")\n",
    "\n",
    "# æ¯ç±»é”™è¯¯ç»Ÿè®¡\n",
    "train_wrong_pairs = [(int(y_true_train[i]), int(y_pred_train[i])) for i in wrong_train_idx]\n",
    "train_wrong_count = Counter(train_wrong_pairs)\n",
    "\n",
    "print(\"\\nğŸ” Train æ¯ç±»é”™è¯¯ç»Ÿè®¡ï¼š(çœŸæ ‡ç­¾ -> é¢„æµ‹æ ‡ç­¾)\")\n",
    "for (t, p), cnt in train_wrong_count.items():\n",
    "    print(f\"  {t} â†’ {p} : {cnt} æ¬¡\")\n",
    "\n",
    "print(\"\\nğŸ Train-only è¯„ä¼°å®Œæˆï¼\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
