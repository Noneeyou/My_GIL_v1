{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca3bf53",
   "metadata": {},
   "source": [
    "# normal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354059c4",
   "metadata": {},
   "source": [
    "## æ•°æ®åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import subgraph, add_self_loops\n",
    "\n",
    "# === å¯¼å…¥æ¨¡å‹ç»“æ„ ===\n",
    "from models import GraphContrastiveLearner, augment_graph, summarize_graph\n",
    "\n",
    "# ===================== åŸºæœ¬è®¾ç½® =====================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/less_class/pyg_data/time/graph_withmask.pt\"\n",
    "save_dir = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model/model_save/time_less_class\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ===================== 2ï¸âƒ£ åŠ è½½ PyG å›¾æ•°æ® =====================\n",
    "data = torch.load(graph_path)\n",
    "summarize_graph(data)\n",
    "\n",
    "# ========== â­ å¯¹ data.x åš Z-score æ ‡å‡†åŒ– ==========\n",
    "x_mean = data.x.mean(dim=0, keepdim=True)\n",
    "x_std = data.x.std(dim=0, keepdim=True) + 1e-6\n",
    "data.x = (data.x - x_mean) / x_std\n",
    "\n",
    "print(\"ğŸ¯ å·²å®Œæˆ data.x æ ‡å‡†åŒ–: meanâ‰ˆ0, stdâ‰ˆ1\")\n",
    "\n",
    "# ===================== 3ï¸âƒ£ æ„é€  batch å­å›¾ =====================\n",
    "batch_size = 1024\n",
    "num_nodes = data.num_nodes\n",
    "\n",
    "perm = torch.randperm(num_nodes)\n",
    "\n",
    "batches = []\n",
    "for i in range(0, num_nodes, batch_size):\n",
    "    node_idx = perm[i:i + batch_size]\n",
    "\n",
    "    # ---- æå–å­å›¾ï¼ˆå¿…é¡» CPUï¼‰----\n",
    "    sub_edge_index, _ = subgraph(\n",
    "        node_idx,\n",
    "        data.edge_index,\n",
    "        relabel_nodes=True\n",
    "    )\n",
    "\n",
    "    sub_x = data.x[node_idx]\n",
    "\n",
    "    # ---------- â­ åŠ å…¥è‡ªç¯ï¼ˆSelf-loopsï¼‰ ----------\n",
    "    sub_edge_index, _ = add_self_loops(\n",
    "        sub_edge_index, num_nodes=sub_x.size(0)\n",
    "    )\n",
    "\n",
    "    sub_data = Data(\n",
    "        x=sub_x,\n",
    "        edge_index=sub_edge_index\n",
    "    )\n",
    "    batches.append(sub_data)\n",
    "\n",
    "print(f\"ğŸ“Œ å…±åˆ’åˆ† batch å­å›¾æ•°é‡ï¼š{len(batches)}\")\n",
    "\n",
    "loader = DataLoader(batches, batch_size=1, shuffle=True)\n",
    "\n",
    "# ===================== 4ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹ =====================\n",
    "in_dim = data.x.size(1)\n",
    "hidden_dim = 256\n",
    "out_dim = 256\n",
    "proj_dim = 128\n",
    "tau = 0.5\n",
    "\n",
    "model = GraphContrastiveLearner(in_dim, hidden_dim, out_dim, proj_dim, tau).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca7d0e",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081294dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 5ï¸âƒ£ Batch è®­ç»ƒ =====================\n",
    "epochs = 200\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "best_model_path = os.path.join(save_dir, f\"KAIST_normal_pretrain_best.pt\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data[0].to(device)  # å› ä¸º batch_size=1\n",
    "\n",
    "        # --- ä¸¤ä»½å¢å¼ºè§†å›¾ ---\n",
    "        data1 = augment_graph(batch_data, feature_drop_prob=0.2, edge_drop_prob=0.1, noise_std=0.02)\n",
    "        data2 = augment_graph(batch_data, feature_drop_prob=0.2, edge_drop_prob=0.1, noise_std=0.02)\n",
    "        data1, data2 = data1.to(device), data2.to(device)\n",
    "\n",
    "        # --- å¯¹æ¯”å­¦ä¹ æŸå¤± ---\n",
    "        loss = model.compute_loss(\n",
    "            data1.x, data1.edge_index,\n",
    "            data2.x, data2.edge_index\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # ======== æ‰“å° =========\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch:03d}/{epochs}] | InfoNCE Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # ======== ä¿å­˜æœ€ä¼˜æ¨¡å‹é€»è¾‘ =========\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"best_loss\": best_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"config\": {\n",
    "                \"in_dim\": in_dim,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"out_dim\": out_dim,\n",
    "                \"proj_dim\": proj_dim,\n",
    "                \"tau\": tau\n",
    "            }\n",
    "        }, best_model_path)\n",
    "\n",
    "        print(f\"ğŸ’¾ [BEST MODEL UPDATED] Epoch {epoch} | Loss={total_loss:.4f}\")\n",
    "\n",
    "# ===================== æœ€ç»ˆæ¨¡å‹ä¹Ÿä¿å­˜ä¸€æ¬¡ï¼ˆéå¿…é¡»ï¼‰ =====================\n",
    "final_path = os.path.join(save_dir, f\"KAIST_normal_pretrain_epoch{epochs}.pt\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"epoch\": epochs,\n",
    "    \"config\": {\n",
    "        \"in_dim\": in_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"out_dim\": out_dim,\n",
    "        \"proj_dim\": proj_dim,\n",
    "        \"tau\": tau\n",
    "    }\n",
    "}, final_path)\n",
    "\n",
    "print(f\"\\nğŸ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {final_path}\")\n",
    "print(f\"ğŸ† æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜è‡³: {best_model_path} | best_loss={best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f936d66",
   "metadata": {},
   "source": [
    "## æ•ˆæœå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "\n",
    "# ==============================================================\n",
    "#  my_CIL_V1/pre_train/KAIST_normal_model/eval_normal_embeddings.ipynb\n",
    "# ==============================================================\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# === å¯¼å…¥æ¨¡å‹å®šä¹‰ ===\n",
    "from models import GraphContrastiveLearner, augment_graph, summarize_graph\n",
    "# ğŸ‘† è¯·æ”¹ä¸ºä½ çš„å®é™…æ¨¡å‹å®šä¹‰æ–‡ä»¶è·¯å¾„\n",
    "# ä¾‹å¦‚: from my_CIL_V1.src.models.graph_contrastive import GraphContrastiveLearner\n",
    "\n",
    "# ===================== 1ï¸âƒ£ å‚æ•°ä¸è·¯å¾„è®¾ç½® =====================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/less_class/pyg_data/time/graph_withmask.pt\"  # ğŸ‘ˆ æ›¿æ¢ä¸ºä½ çš„è·¯å¾„\n",
    "model_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model/model_save/time_less_class/KAIST_normal_pretrain_best.pt\"  # ğŸ‘ˆ æ¨¡å‹è·¯å¾„\n",
    "save_dir = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model/result/time_less_class\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ===================== 2ï¸âƒ£ åŠ è½½å›¾æ•°æ®ä¸æ¨¡å‹ =====================\n",
    "data = torch.load(graph_path)\n",
    "data = data.to(device)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "cfg = checkpoint[\"config\"]\n",
    "\n",
    "model = GraphContrastiveLearner(**cfg).to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
    "\n",
    "# ===================== 3ï¸âƒ£ æå–åµŒå…¥ =====================\n",
    "with torch.no_grad():\n",
    "    h, z = model(data.x, data.edge_index)  # h=ç¼–ç ç‰¹å¾, z=æŠ•å½±ç‰¹å¾\n",
    "\n",
    "# åªä¿ç•™ test_mask ä¸‹çš„èŠ‚ç‚¹\n",
    "mask = data.test_mask.cpu().numpy()\n",
    "\n",
    "embeddings = z.cpu().numpy()[mask]\n",
    "\n",
    "# ---- åˆ¤æ–­ y æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ ----\n",
    "if hasattr(data, \"y\") and data.y is not None:\n",
    "    labels = data.y.cpu().numpy()[mask]\n",
    "else:\n",
    "    labels = np.zeros(mask.sum(), dtype=int)\n",
    "    print(\"âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„æ ‡ç­¾ data.yï¼Œå°†ä½¿ç”¨å…¨é›¶æ ‡ç­¾ä»£æ›¿ã€‚\")\n",
    "\n",
    "print(f\"ğŸ§© åµŒå…¥ç»´åº¦: {embeddings.shape}\")\n",
    "\n",
    "\n",
    "print(f\"ğŸ§© åµŒå…¥ç»´åº¦: {embeddings.shape}\")\n",
    "\n",
    "# ===================== 4ï¸âƒ£ UMAP é™ç»´ =====================\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=10, \n",
    "    min_dist=0.1, \n",
    "    metric='cosine', \n",
    "    random_state=42\n",
    ")\n",
    "emb_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "# ===================== 5ï¸âƒ£ ç»˜åˆ¶åµŒå…¥å›¾ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ =====================\n",
    "plt.figure(figsize=(12,8))\n",
    "if labels is not None:\n",
    "    scatter = plt.scatter(\n",
    "        emb_2d[:,0], emb_2d[:,1],\n",
    "        c=labels, cmap='tab20', s=10, alpha=0.8\n",
    "    )\n",
    "    # ğŸ”§ å›¾ä¾‹æ”¹ä¸ºå³ä¾§å¤–ç½®ï¼Œé¿å…è¦†ç›–æ•°æ®\n",
    "    legend = plt.legend(\n",
    "        *scatter.legend_elements(num=None),\n",
    "        title=\"Labels\",\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        fontsize=8\n",
    "    )\n",
    "    plt.gca().add_artist(legend)\n",
    "else:\n",
    "    plt.scatter(emb_2d[:,0], emb_2d[:,1], s=10, alpha=0.8)\n",
    "\n",
    "plt.title(\"UMAP Projection of GCL Embeddings (Test Mask)\")\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])  # ä¸ºå›¾ä¾‹è…¾å‡ºç©ºé—´\n",
    "\n",
    "fig_path = os.path.join(save_dir, \"KAIST_normal_umap.png\")\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ åµŒå…¥å¯è§†åŒ–å›¾å·²ä¿å­˜ï¼š{fig_path}\")\n",
    "\n",
    "# ===================== 6ï¸âƒ£ è®¡ç®—è¡¨å¾æ€§èƒ½æŒ‡æ ‡ =====================\n",
    "def compute_gcl_metrics(embeddings, labels):\n",
    "    \"\"\"\n",
    "    è®¡ç®—ä¸ GCL è¡¨å¾è´¨é‡ç›¸å…³çš„å‡ é¡¹æŒ‡æ ‡ï¼š\n",
    "      - Silhouette Scoreï¼ˆèšç±»åˆ†ç¦»åº¦ï¼‰â†’ è¶Šé«˜è¶Šå¥½\n",
    "      - Feature Uniformityï¼ˆç»´åº¦æ–¹å·®å‡åŒ€æ€§ï¼‰â†’ è¶Šé«˜è¶Šå¥½\n",
    "      - Mean Normï¼ˆç‰¹å¾å¹³å‡èŒƒæ•°ï¼‰â†’ è¶Šé«˜è¡¨ç¤ºåˆ†å¸ƒå¹…åº¦æ›´å¤§\n",
    "      - Std Normï¼ˆç‰¹å¾èŒƒæ•°æ ‡å‡†å·®ï¼‰â†’ è¶Šä½è¶Šå¥½ï¼ˆç¨³å®šæ€§ï¼‰\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    if labels is not None and len(np.unique(labels)) > 1:\n",
    "        results[\"silhouette\"] = float(silhouette_score(embeddings, labels))\n",
    "    else:\n",
    "        results[\"silhouette\"] = None\n",
    "\n",
    "    norms = np.linalg.norm(embeddings, axis=1)\n",
    "    results[\"mean_norm\"] = float(norms.mean())\n",
    "    results[\"std_norm\"] = float(norms.std())\n",
    "\n",
    "    var_per_dim = np.var(embeddings, axis=0)\n",
    "    results[\"feature_uniformity\"] = float(var_per_dim.mean())\n",
    "\n",
    "    return results\n",
    "\n",
    "metrics = compute_gcl_metrics(embeddings, labels)\n",
    "\n",
    "print(\"\\nğŸ” GCL è¡¨å¾æ€§èƒ½æŒ‡æ ‡ (å«è§£é‡Š)ï¼š\")\n",
    "desc = {\n",
    "    \"silhouette\": \"èšç±»åˆ†ç¦»åº¦ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰\",\n",
    "    \"mean_norm\": \"ç‰¹å¾å¹³å‡èŒƒæ•°ï¼ˆè¶Šé«˜ä»£è¡¨å¹…åº¦æ›´å¤§ï¼‰\",\n",
    "    \"std_norm\": \"èŒƒæ•°æ ‡å‡†å·®ï¼ˆè¶Šä½è¶Šç¨³å®šï¼‰\",\n",
    "    \"feature_uniformity\": \"ç»´åº¦æ–¹å·®å‡åŒ€æ€§ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰\"\n",
    "}\n",
    "for k, v in metrics.items():\n",
    "    if v is not None:\n",
    "        print(f\"  {desc[k]:<20s}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {desc[k]:<20s}: N/A\")\n",
    "\n",
    "# ===================== 7ï¸âƒ£ ä¿å­˜ç»“æœ =====================\n",
    "np.save(os.path.join(save_dir, \"KAIST_normal_emb_test.npy\"), embeddings)\n",
    "np.save(os.path.join(save_dir, \"KAIST_normal_umap_test.npy\"), emb_2d)\n",
    "with open(os.path.join(save_dir, \"KAIST_normal_metrics.txt\"), \"w\") as f:\n",
    "    for k, v in metrics.items():\n",
    "        f.write(f\"{desc[k]}: {v}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… å…¨éƒ¨ç»“æœå·²ä¿å­˜åˆ°ï¼š{save_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954c782",
   "metadata": {},
   "source": [
    "## ä¸‹æ¸¸åˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1379ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²æ·»åŠ è·¯å¾„ï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model\n",
      "âœ… å·²æ·»åŠ  my_libï¼š/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_691895/2756859630.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(graph_path).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ§© Graph Data Summary\n",
      "============================================================\n",
      "ğŸ“Š èŠ‚ç‚¹æ•°é‡ (num_nodes): 12000\n",
      "ğŸ“ˆ èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ (num_features): 4\n",
      "ğŸ”— è¾¹æ•°é‡ (num_edges): 240000\n",
      "ğŸ” è‡ªç¯æ•°é‡ (self-loops): 0\n",
      "ğŸ¯ æ ‡ç­¾ç»´åº¦ (y_dim): torch.Size([12000])\n",
      "\n",
      "ğŸ§¾ Dataå¯¹è±¡åŒ…å«å­—æ®µ: ['train_withlabel_mask', 'edge_index', 'val_mask', 'x', 'test_mask', 'y', 'train_nolabel_mask', 'train_mask']\n",
      "============================================================\n",
      "\n",
      "âœ… ä¸Šæ¸¸ GCL ç¼–ç å™¨åŠ è½½å®Œæˆ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_691895/2756859630.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ä¸‹æ¸¸è®­ç»ƒæ ·æœ¬æ•°: 2880\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 1, Acc=0.5229\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 2, Acc=0.5674\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 3, Acc=0.6049\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 4, Acc=0.6253\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 5, Acc=0.6521\n",
      "Epoch 005 | Loss=11.6475 | Train Acc=0.6521\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 6, Acc=0.6819\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 7, Acc=0.6951\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 8, Acc=0.6993\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 9, Acc=0.7194\n",
      "Epoch 010 | Loss=9.3155 | Train Acc=0.7128\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 11, Acc=0.7226\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 13, Acc=0.7260\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 14, Acc=0.7271\n",
      "Epoch 015 | Loss=8.6664 | Train Acc=0.7201\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 16, Acc=0.7288\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 19, Acc=0.7431\n",
      "Epoch 020 | Loss=8.1007 | Train Acc=0.7424\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 21, Acc=0.7438\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 22, Acc=0.7444\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 23, Acc=0.7510\n",
      "Epoch 025 | Loss=7.6559 | Train Acc=0.7503\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 28, Acc=0.7528\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 29, Acc=0.7556\n",
      "Epoch 030 | Loss=7.4649 | Train Acc=0.7479\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 31, Acc=0.7594\n",
      "Epoch 035 | Loss=7.3982 | Train Acc=0.7472\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 38, Acc=0.7611\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 40, Acc=0.7649\n",
      "Epoch 040 | Loss=7.3206 | Train Acc=0.7649\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 41, Acc=0.7684\n",
      "Epoch 045 | Loss=7.1248 | Train Acc=0.7611\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 47, Acc=0.7719\n",
      "Epoch 050 | Loss=6.9474 | Train Acc=0.7653\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 53, Acc=0.7747\n",
      "Epoch 055 | Loss=6.9117 | Train Acc=0.7747\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 57, Acc=0.7771\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 58, Acc=0.7823\n",
      "Epoch 060 | Loss=6.6738 | Train Acc=0.7792\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 63, Acc=0.7833\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 65, Acc=0.7844\n",
      "Epoch 065 | Loss=6.6249 | Train Acc=0.7844\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 67, Acc=0.7851\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 70, Acc=0.7892\n",
      "Epoch 070 | Loss=6.5307 | Train Acc=0.7892\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 72, Acc=0.7896\n",
      "Epoch 075 | Loss=6.3637 | Train Acc=0.7802\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 77, Acc=0.7948\n",
      "Epoch 080 | Loss=6.2463 | Train Acc=0.7875\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 81, Acc=0.7951\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 83, Acc=0.7958\n",
      "Epoch 085 | Loss=6.1551 | Train Acc=0.7899\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 88, Acc=0.7972\n",
      "Epoch 090 | Loss=6.0427 | Train Acc=0.7840\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 94, Acc=0.8021\n",
      "Epoch 095 | Loss=6.0156 | Train Acc=0.7910\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 97, Acc=0.8056\n",
      "ğŸ’¾ [BEST UPDATED] Epoch 98, Acc=0.8076\n",
      "Epoch 100 | Loss=5.9706 | Train Acc=0.8017\n",
      "\n",
      "ğŸ ä¸‹æ¸¸åˆ†ç±»è®­ç»ƒå®Œæˆ\n",
      "ğŸ”¥ æœ€ä½³åˆ†ç±»å™¨ acc=0.8076\n",
      "ğŸ’¾ å·²ä¿å­˜è‡³: /home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model/model_save/time_less_class/downstream/best_classifier.pt\n"
     ]
    }
   ],
   "source": [
    "# ================== 1ï¸âƒ£ å¯¼å…¥è·¯å¾„ä¸å‡½æ•° ==================\n",
    "%run ../_init_path.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# === å¯¼å…¥ä¸Šæ¸¸æ¨¡å‹ ===\n",
    "from models import GraphContrastiveLearner, summarize_graph\n",
    "\n",
    "# === å¯¼å…¥ä¸‹æ¸¸åˆ†ç±»å¤´ ===\n",
    "from models import DownstreamClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "graph_path = \"/home/charles/HZU/Data_processed/my_CIL_V1/KAIST/less_class/pyg_data/time/graph_withmask.pt\"\n",
    "model_path = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model/model_save/time_less_class/KAIST_normal_pretrain_best.pt\"\n",
    "save_dir = \"/home/charles/HZU/Industrial_Software_Testing/Industrial_Software_Testing/my_CIL_V1/GCL_pre_train/KAIST_normal_model/model_save/time_less_class/downstream\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ================== 2ï¸âƒ£ åŠ è½½æ•°æ® + ä¸Šæ¸¸æ¨¡å‹ ==================\n",
    "data = torch.load(graph_path).to(device)\n",
    "summarize_graph(data)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "cfg = checkpoint[\"config\"]\n",
    "\n",
    "model = GraphContrastiveLearner(**cfg).to(device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… ä¸Šæ¸¸ GCL ç¼–ç å™¨åŠ è½½å®Œæˆ\")\n",
    "\n",
    "# ================== 3ï¸âƒ£ FULL GRAPH è¿è¡Œ encoderï¼ˆæ­£ç¡®æ–¹å¼ï¼‰ ==================\n",
    "with torch.no_grad():\n",
    "    h_all, _ = model(data.x, data.edge_index)   # å¿…é¡»ç”¨æ•´ä¸ªå›¾\n",
    "\n",
    "# ================== 4ï¸âƒ£ ä½¿ç”¨ train_withlabel_mask æ„é€ è®­ç»ƒé›† ==================\n",
    "mask = data.train_withlabel_mask.cpu()\n",
    "train_idx = mask.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "train_h = h_all[train_idx].detach()          # (N_train, hidden_dim)\n",
    "train_y = data.y[train_idx].cpu()            # (N_train,)\n",
    "\n",
    "train_dataset = TensorDataset(train_h.to(device), train_y.to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "print(f\"ğŸ“Œ ä¸‹æ¸¸è®­ç»ƒæ ·æœ¬æ•°: {len(train_idx)}\")\n",
    "\n",
    "# ================== 5ï¸âƒ£ åˆå§‹åŒ–åˆ†ç±»å™¨ ==================\n",
    "num_classes = int(data.y.max().item() + 1)\n",
    "in_dim = train_h.size(1)\n",
    "hidden_dim = 128\n",
    "\n",
    "classifier = DownstreamClassifier(in_dim, num_classes, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# ================== 6ï¸âƒ£ è®­ç»ƒä¸‹æ¸¸åˆ†ç±»å™¨ ==================\n",
    "epochs = 100\n",
    "best_acc = 0\n",
    "best_path = os.path.join(save_dir, \"best_classifier.pt\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    classifier.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_h, batch_y in train_loader:\n",
    "        logits = classifier(batch_h)\n",
    "        loss = criterion(logits, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # ======== è®¡ç®— train acc ========\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_all = classifier(train_h.to(device))\n",
    "        pred_all = logits_all.argmax(dim=1)\n",
    "        acc = (pred_all == train_y.to(device)).float().mean().item()\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(classifier.state_dict(), best_path)\n",
    "        print(f\"ğŸ’¾ [BEST UPDATED] Epoch {epoch}, Acc={acc:.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss={total_loss:.4f} | Train Acc={acc:.4f}\")\n",
    "\n",
    "# ================== ä¿å­˜æœ€ç»ˆæ¨¡å‹ ==================\n",
    "final_path = os.path.join(save_dir, \"final_classifier.pt\")\n",
    "torch.save(classifier.state_dict(), final_path)\n",
    "\n",
    "print(\"\\nğŸ ä¸‹æ¸¸åˆ†ç±»è®­ç»ƒå®Œæˆ\")\n",
    "print(f\"ğŸ”¥ æœ€ä½³åˆ†ç±»å™¨ acc={best_acc:.4f}\")\n",
    "print(f\"ğŸ’¾ å·²ä¿å­˜è‡³: {best_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
